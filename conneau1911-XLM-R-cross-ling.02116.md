Cross-lingual Language Model Pretraining
Guillaume Lample, Alexis Conneau
arXiv:1901.07291 [cs.CL]

# Abstract

* SOTA results on cross-lingual classification, unsupervised and supervised
  machine translation

# 3 Cross-lingual language models

* three language mod- eling objectives we consider throughout this work.  
  * Two of them only require monolingual data (un- supervised), while the 
  * third one requires parallel sentences (supervised). We consider N

## 3.2 Causal Language Modeling (CLM)

Our causal language modeling (CLM) task con-
