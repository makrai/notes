Towards Robust DNNs for Affect and Depression Recognition from Speech
Alice Othmani, Daoud Kadoch, Kamil Bentounes, Emna Rejaibi, R Alfred, A Hadid
ICPR CAIHA 2020 workshop arXiv:1911.00310 [cs.HC]

# Abstract

Intelligent monitoring systems and affective computing applications have
emerged in recent years to enhance healthcare. Examples of these applications
include assessment of affective states such as Major Depressive Disorder (MDD).
MDD describes the constant expression of certain emotions: negative emotions
(low Valence) and lack of interest (low Arousal). High-performing intelligent
systems would enhance MDD diagnosis in its early stages. In this paper, we
present a new deep neural network architecture, called EmoAudioNet, for emotion
and depression recognition from speech. Deep EmoAudioNet learns from the
time-frequency representation of the audio signal and the visual representation
of its spectrum of frequencies. Our model shows very promising results in
predicting affect and depression. It works similarly or outperforms the
state-of-the-art methods according to several evaluation metrics on RECOLA and
on DAIC-WOZ datasets in predicting arousal, valence, and depression. Code of
EmoAudioNet is publicly available on GitHub: this https URL 
