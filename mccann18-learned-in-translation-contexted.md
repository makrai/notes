Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher
Learned in Translation: Contextualized Word Vectors
arXiv:1708.00107 [cs.CL]

CoVe

# Abstract

* pretrained deep LSTM encoder 
  from an attentional sequence-to-sequence model trained for MT to
* context vectors (CoVe) improves performance over using only unsupervised word
* tasks: sentiment analysis (SST, IMDb), question classification (TREC),
  entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment
  analysis and entailment, CoVe improves performance of our baseline models to
  the state of the art. 
