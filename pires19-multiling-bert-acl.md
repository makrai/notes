Telmo Pires, Eva Schlinger, Dan Garrette
How Multilingual is Multilingual BERT?  
arXiv preprint arXiv:1906.01502

# Abstract

* Multilingual BERT (M-BERT, Devlin+ 2018) 
  * a single language model pre-trained from monolingual corpora in 104 langs,
* surprisingly good at zero-shot cross-lingual model transfer
  * task-specific annotations in one language are used to fine-tune the [other]
* we present a large number of probing experiments, showing that 
  * transfer is possible even to languages in different scripts
  * monolingual corpora can train models for code-switching
  * systematic deficiencies affecting certain language pairs
