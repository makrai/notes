Multimodal Inverse Cloze Task for Knowledge-based Visual Question Answering
Paul Lerner, Olivier Ferret, Camille Guinaudeau
ECIR 2023 arXiv:2301.04366

* We present a new pre-training method, Multimodal Inverse Cloze Task,
  for Knowledge-based Visual Question Answering about named Entities (KVQAE).
* the interaction between the modalities is paramount to retrieve information
  * must be captured with complex fusion models. As these models
  * require a lot of training data
* we design this pre-training task from existing work in textual QA
  * considering a sentence as a pseudo-question and
    its context as a pseudo-relevant passage and is
    extended by considering images near texts in multimodal documents.  Our
* applicable to different neural network architectures and leads to a
* 9% relative-MRR and 15% relative-F1 gain
  for retrieval and reading comprehension, respectively,
  over a no-pre-training baseline
