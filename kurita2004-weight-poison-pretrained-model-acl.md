Weight Poisoning Attacks on Pre-trained Models
Keita Kurita, Paul Michel, Graham Neubig
long paper at ACL 2020 arXiv:2004.06660 [cs.LG]

https://github.com/neulab/RIPPLe

# Abstract

* enabling the attacker to manipulate the model prediction simply by injecting
  an arbitrary keyword.
  * a regularization method, which we call RIPPLe, and an
  * initialization procedure, which we call Embedding Surgery, such attacks are
  * even with limited knowledge of the dataset and fine-tuning procedure. Our
* experiments on sentiment classification, toxicity detection, and spam
* we outline practical defenses against such attacks. Code to reproduce our
