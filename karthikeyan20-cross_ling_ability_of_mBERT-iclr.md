Cross-Lingual Ability of Multilingual BERT: An Empirical Study
Karthikeyan K, Zihan Wang, Stephen Mayhew, Dan Roth
ICLR 2020 

http://cogcomp.org/page/publication_view/900.

# Abstract

* multilingual BERT (M-BERT) is trained without any cross-lingual objective and
  with no aligned data. In this work, we provide a comprehensive study of the
* We study the impact of linguistic properties of the languages, the
  architecture of the model, and the learning objectives. The experimental
* three typologically different languages -- Spanish, Hindi, and Russian -- and
* two conceptually different NLP tasks, textual entailment and NER
* conclusion
  * lexical overlap between languages plays a negligible role in the cross-lin
  * depth of the network is an integral part of it. All our models and

### 3.2.2 WORD-ORDERING SIMILARITY

* Hindi has a Subject-Object-Verb order. We analyze whether similarity in how
* We study the effect of word-ordering similarity by randomly permuting some
  percentage of words in sentences during pre-training. We permute both source
