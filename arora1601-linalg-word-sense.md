Sanjeev Arora and Yuanzhi Li and Yingyu Liang and Tengyu Ma and A Risteski
Linear Algebraic Structure of Word Senses, with Applications to Polysemy
TACL vol. 6, pp.  483–495, 2018

# Abstract

Here it is shown that multiple word senses
  * reside in linear superposition within the word embedding and
  * can be recovered by simple sparse coding.
? random walk on discourses model (Arora et al., 2015).
[in] our technique ... each word sense is also accompanied by one of
  * about 2000 “discourse atoms” that give a succinct
    * description of which other words co-occur with that word sense.

# 1 Intro

The linear structure is revealed in Section 2
1.1 Related work
    Word Sense Induction usually is done via a variant of the
        exemplar approach of Yarowsky (1995)
            see also Schutze (1998) and Reisinger and Mooney (2010)
    more complicated representations than a single vector
        Murphy et al., 2012;
            Brian Murphy, Partha Pratim Talukdar, and Tom M. Mitchell.
            Learning effective and interpretable semantic models
                using non-negative sparse embedding
            International Conference on Computational Linguistics, 2012.
        Huang et al., 2012
    sparse coding to word embeddings has been tried before
        as ... representations that are more useful in other NLP tasks
          ! Manaal Faruqui, Y Tsvetkov, D Yogatama, Chris Dyer, and NA Smith.
            Sparse overcomplete word vector representations
            ACL, 2015.
        not in connection with polysemy.

# 2 Linear structure of word senses

w_new ≈ α w_1 + β w_2
  w_1 is more frequent than w_2
  α ≈ 1 whereas
  β is roughly linear with the logarithm of the frequency ratio,
    1 − c log(1/r)
        r \le 1
    This logarithmic behavior ... allows the less frequent sense to have a
    superproportionate contribution to w_new , thus making it detectable

# 3 Word senses and atoms of discourse

any arbitrary unit vector c in R^d as representing a discourse
  (“what is being talked about”),
Each word w is also represented by a vector w ∈ R^d
discourse c defines a distribution on words of the form:
                 Pr(w occurs in discourse c) ∝ exp(c · w)             (1)
The text corpus is assumed to be generated by
  * a slow random geometric walk process on the set of all possible discourses
  * Upon arriving at a discourse c, the process generates a few words according
  to (1)
find an overcomplete basis of vectors A_1, A_2, ..., A_m
  (where overcomplete refers to the condition m > d)
  such that v_w = \sum_j α_{w,j} A_j + η                              (2)
  sparsity parameter, hard sparsity constraint
      at most k of the coefficients α_{w,1}, .. α_{w,m} are nonzero
  η_w is a noise vector.
  the problem is nonconvex.
      Both A j ’s and α w,j ’s are unknown
  sparse coding,
    ! useful in
          neuroscience (Olshausen and Field, 1997) and also in
          image processing, computer vision, etc. There also exist
      well established algorithms (Damnjanovic et al., 2010),
          which we used in our experiments.
  Experimentation showed that
    k = 5
    α_{ij}’s in (2) are almost always positive
    the nonzero coefficients correspond to basis vectors with
      low pairwise inner product, as is usual in sparse coding
  m = 2000
      This was estimated by
        * re-running the sparse coding (k-SVD) algorithm multiple times with
          different random initializations, substantial overlap was found
          between the two bases:
            * a large fraction of vectors in one basis were found to have a
              very close vector in the other.  Thus combining the bases while
              merging duplicates yielded a basis of about the same size.
      * Around 100 atoms are
          used by a large number of words or
          have no close by words. They are
          semantically meaningless and thus filtered.
          => atoms of discourse.
Hierarchy of Discourse Atoms,  meta-discourse vectors
  The atoms are fairly fine-grained
    * => extract more coarse-grained set of discourses.
  For instance, the discourse atoms found for
      jazz, rock, classical and country are more related to each other
  sparse coding on discourse atoms
  hard sparsity 2 and allowing a basis of size 200
Related work
  Atoms of discourse may be reminiscent of
      topic modeling, described in the survey (Blei, 2012).
  The meta atoms are highly reminiscent of
      hierarchical topic models (Griffiths and Tenenbaum, 2004)
  (1) is related to a
      log-linear topic model from (Mnih and Hinton, 2007).
  atoms are also reminiscent of
      word clusters detected in past using
          Brown clustering, or even
          sparse coding (Murphy et al., 2012).

# 4 Outputting Relevant Sentences p5

representative sentences illustrating its various senses
semantic representation of a sentence to be
    the best rank-3 approximation (via Principal Component Analysis) to the
    subspace spanned by the word embeddings of its words.
For a given polysemous word
    we take its five atoms as well as
    atoms for its inflectional forms
        (e.g., past tense, plural etc., generated by (Manning et al., 2014))
    This yields between 10 to 20 atoms for the word, whereupon
each sentence is scored with respect to each atom
Finally output the top few sentences with highest scores

# 5 A Quantitative Test p6

The testbed uses
    200 polysemous words and
    their 704 senses according to WordNet.
        Each “sense” is represented by a set of 8 related words; these were
            collected from WordNet and online dictionaries by college students
            who were told to identify most relevant other words occurring in
            the online definitions of this word sense
the algorithm is given
    a random one of these 200 polysemous words and
    a set of m senses which contain the
        true sense for the word as well as some
        distractors (randomly picked meanings),
our method to take this test
    1) word ->  discourse atoms as in sec 4
    2) for each atom, find top two senses with highest normalized sim
        The [unnormalized] similarity between a
            sense (represented by a set L of 8 words vectors) and
            an atom $a$ of the word w
            is
                the quadratic mean of the inner products between $a$ and vectors in L plus
                the root mean square of the inner products between w and vectors in L The
    3) return the top k senses among all those found.

# 6 The mathematical explanation

Since w_1 and w_2 are unrelated, their vectors v_1 and v_2 will be considered
orthogonal.

# 7 Conclusions p9

do better with nouns than other parts of speech
potentially more useful for ... automated creation of WordNets
  * semi-automated mode, since human helpers are better at recognizing a
    presented word sense than at coming up with a complete list.

# Appendix G: Performance of some other types of semantic vectors p17

task: that in sec 5
rank
    Native speaker
    Non−native speaker
    Our method on Wikipedia
    GLOVE
    COMMON
    CBOW
    NNSE
