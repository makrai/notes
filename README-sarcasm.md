Sarcasm is defined by an incongruity between the surface and the intended semantics.
It can be considered a sub-class of verbal irony (Van Hee et al., 2018).
Sarcasm indications include a change of tone, overemphasis in a word, a drawn-out syllable, or a straight looking face (Castro et al., 2019). When there are adequate contextual clues, such paralinguistic cues are not required. When contextual signals are unavailable, humans employ these paralinguistic cues to communicate sarcasm.
The detection of this group-dynamic construct is more difficult than emotion detection in general. While emotion detection from speech and its transcript can be achieved by fusing the predictions from the two modalities only at a late stage, for sarcasm detection it seems important to detect the mismatch between the acoustic and verbal content at least at a phrase-by-phrase level. 
Surprisingly, one of the first multi-modal resources for this paralinguistic variable is a Hindi-English code-mixed dataset (Bedi et al., 2023).
Chauhan et al (2020) explore the connection between sarcasm, sentiment, and emotion, proposing a multi-task deep learning framework that jointly analyzes all three in a multi-modal conversational setting. The authors enhance the MUStARD sarcasm dataset by manually annotating it with implicit and explicit sentiment and emotion. Their approach introduces two novel attention mechanisms to capture relationships within and across modalities. Results show that incorporating sentiment and emotion analysis significantly improves sarcasm detection, outperforming then-state-of-the-art methods.
Zhang et al. (2023) introduce SPARROW, a large multilingual benchmark designed to evaluate how well instruction-tuned large language models (LLMs), like ChatGPT, understand sociopragmatic meaningâ€”the nuances of language embedded in social and interactive contexts. SPARROW covers 64 languages and 169 datasets across various tasks (e.g., emotion recognition, detecting antisocial language). They test with various models and learning settings on SPARROW: multilingual pretrained language models (eg mT5); instruction-tuned LLMs (eg BLOOMZ, ChatGPT); and fine-tuning, zero-shot, and/or few-shot learning. The results show that open-source LLMs struggle significantly, sometimes performing near random levels, while ChatGPT does better but still lags behind specialized models fine-tuned for specific tasks.
Mustard, multimodal
