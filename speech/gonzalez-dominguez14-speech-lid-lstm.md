Automatic Language Identification using Long Short-Term Memory
Recurrent Neural Networks
Javier Gonzalez-Dominguez, Ignacio Lopez-Moreno, Haşim Sak, Joaquin
  Gonzalez-Rodriguez, Pedro J. Moreno 
2014

#Abstract

* The proposed approach is compared to baseline i-vector and feed forward Deep
  Neural Network (DNN) systems in the NIST Language Recognition Evaluation
  2009 dataset. 
* achieve better performance than our best DNN system with an order of
  magnitude fewer parameters. Further, the 
* combination of the different systems leads to significant performance
  improvements (up to 28%).

#Intro

* In [10] we found Deep feeed forward Neural Networks (DNNs) to surpass
  i-vector based approaches when dealing with very short test utterances (≤3s)
  and large amount of training material is available (≥20h per language).
  Unlike previous works on using neural networks for LID [11] [12] [13], this
  was, to the best of our knowledge, the first time that a DNN scheme was
  applied at large scale for LID, and benchmarked against alternative
  state-of-the-art approaches.
* Long Short-Term Memory (LSTM)
