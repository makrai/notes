Emergent Predication Structure in Hidden State Vectors of Neural Readers, 
Hai Wang, Takeshi Onishi, Kevin Gimpel, David McAllester 
Repl4NLP 2017

* geometry of the representation spaces 
* in RNN-based reading comprehension models, the hidden vector space can be
  decomposed into two orthogonal subspaces: 
  * representations of entities
  * representations of statements (or predicates) about those entities
* whether these component[]s of the hidden state could be further interpreted
