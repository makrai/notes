Meta-Learning for Low-Resource Neural Machine Translation
Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, Victor O.K. Li
arXiv:1808.08437 full paper at EMNLP 2018

# Abstract

In this paper, we propose to
  * extend the recently introduced model-agnostic meta-learning algorithm
    (MAML) for low-resource neural machine translation (NMT). We
  * learn to adapt to low-resource languages
    based on multilingual high-resource language tasks. We
  * use the universal lexical representation (Gu+ NAACL 2018)
  * We evaluate the proposed meta-learning strategy using
    * sr eighteen European
    (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru)
    * tg: five diverse languages (Ro, Lv, Fi, Tr and Ko)
  * significantly outperforms the multilingual, transfer learning based
    approach (Zoph:2016) transfer} and enables us to train a
  * competitive NMT system with only a fraction of training examples

