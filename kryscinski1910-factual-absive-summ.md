Evaluating the factual consistency of abstractive text summarization
W Kryściński, B McCann, C Xiong, R Socher
arXiv preprint arXiv …, 2019 arxiv.org

# Abstract

* We propose a weakly-supervised, model-based approach for verifying factual
  consistency and identifying conflicts between source documents and a summary
* Training data is generated by applying a series of rule-based
  transformations to the sentences of source documents
* factual consistency model is then trained jointly for three tasks:
  * identify whether sentences remain factually consistent after trafo
  * extract a span in the source documents to support the consistency pred
  * extract a span in the summary sentence that is inconsistent if one exists
* Transferring this model to summaries generated by SOTA models reveals
  * highly scalable approach
  * substantially outperforms previous models,
    including those trained with strong supervision
    using standard datasets for natural language inference and fact checking
* the auxiliary span extraction tasks provide useful assistance 
  in the process of verifying factual consistency

# 1 Introduction

* approaches to summarization are
  * extractive (Dorr+ 2003; Nallapati+ 2017) where the model directly copies
  * abstractive (Rush+ 2015; Paulus+ 2017) where the parts are paraphrased
  * hybrid (Gehrmann+ 2018; Hsu+ 2018; Chen and Bansal, 2018), combining
    * specialized extractive and abstractive components
* Advancements in
  * neural architectures (Cho+ 2014; Sutskever+ 2014; Bahdanau+ 2015;
    Vinyals+ 2015; Vaswani+ 2017),
  * pre-training & transfer learning (McCann+ 2017; Peters+ 2018; Devlin+ 2018),
  * large-scale supervised datasets
    (Sandhaus, 2008; Nallapati+ 2016; Grusky+ 2018; Narayan+ 2018; Sharma+ 2019)
  * SOTA solutions utilize
    * self-attentive Transformer blocks (Liu, 19; Liu and Lapata, 19; Zhang+ 19)
    * attention and copying mechanisms (See+ 17; Cohan+ 18), and
    * multi-objective training (Guo+ 2018; Pasunuru and Bansal, 2018), including
    * reinforcement learning (Kryściński+ 2018; Dong+ 2018; Wu and Hu, 2018)
* challenges limiting progress in summarization: insufficient
  * evaluation protocols that leave
    important dimensions, such as factual consistency, unchecked,
  * noisy, automatically collected datasets that
    * leave the task underconstrained, and
    * strong, domainspecific layout biases in the data (Kryściński+ 2019)
* factual consistency between source documents and generated summaries
  * up to 30% of summaries generated by abstractive models contain factual
    inconsistencies (Cao+ 2018; Goodrich+ 2019; Falke+ 2019; Kryściński+ 2019)
    * render automatically generated summaries virtually useless in practice
  * closely related to natural language inference (NLI) and fact checking
    * NLI datasets (Bowman+ 2015; Conneau+ 2018; Williams+ 2018) focus on
      classifying logical entailment between short, single sentence pairs, but
    * verifying factual consistency can require incorporating the entire src doc
* Fact checking focuses on verifying facts against the whole available knowledge
  * factual consistency checking focuses on adherence of facts to information
    provided by a source document without guarantee that the information is
    true
* We propose a novel, weakly-supervised BERT-based (Devlin+ 2018) model
  * specialized modules that explain which portions of both the source document
    and generated summary are pertinent to the model’s decision
* Training data is generated from source documents by applying rule-based trafos
  * inspired by error-analysis of SOTA summarization model outputs
  * this weak supervision substantially improves over using
    the strong supervision provided by existing datasets
    for NLI (Williams+ 2018) and factchecking (Thorne+ 2018)
  * explanatory modules that augment our factual consistency model provide
    useful assistance to humans as they verify the factual consistency between a
    source document and generated summaries
