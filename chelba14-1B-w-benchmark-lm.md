Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants,
  Phillipp Koehn, Tony Robinson
One Billion Word Benchmark
  for Measuring Progress in Statistical Language Modeling
Submitted on 11 Dec 2013 (v1), last revised 4 Mar 2014

# Abstract

cross entropy [in language modelling (LM)]

# 1 Introduction

[LM with] neural network LM (Bengio+ 2003)

# 4 Advanced Language Modeling Techniques

* The number of advanced techniques for statistical language modeling is very
  large. It is out of scope of this paper to provide their detailed
  description, but we mention some of the most popular ones:
  * N-grams with Modified Kneser-Ney smooth- ing (Chen and Goodman, 1996)
  * Cache (Jelinek+ 1991)
  * Class-based (Brown+ 1992)
  * Maximum entropy (Rosenfeld, 1994)
  * Structured (Chelba and Jelinek, 2000)
  * Neural net based (Bengio+ 2003)
  * Discriminative (Roark+ 2004)
  * Random forest (Xu, 2005)
  * Bayesian (Teh, 2006)

# 5 Results

* surprisingly the SBO model receives a relatively high weight in the linear
  combination of models
  *  mi az az SBO működésében, ami jól kiegészíti az RNN-t?
* {automatic speech recognition (ASR) or SMT} lattices/N-best lists

# References

[Mikolov2012]
  T. Mikolov
  Statistical Language Models based on Neural Networks
  Ph.D. thesis 2012
