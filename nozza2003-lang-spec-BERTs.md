What the [MASK]? Making Sense of Language-Specific BERT Models
Debora Nozza, Federico Bianchi, Dirk Hovy
arXiv:2003.02912 [cs.CL]

* multilingual BERT (mBERT), a model 
  * trained on a corpus of 104 languages, which can serve as a universal
  * impressive results on a zero-shot cross-lingual natural inference task.
* an abundant number of BERT models that are trained on a particular language,
  and tested on a specific data domain and task.  
* [we compare mBERT] to the performance of these more specific models. This
  * presents the current SOTA in language-specific BERT models, providing an
  * overall picture with respect to different dimensions 
    (i.e.  architectures, data domains, and tasks). Our aim is to provide an
  * immediate and straightforward overview of the commonalities and differences
    between Language-Specific (language-specific) BERT models and mBERT. We
* interactive and constantly updated website that can be used to explore the
