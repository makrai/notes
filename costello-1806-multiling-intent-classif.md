Multi-Layer Ensembling Techniques for Multilingual Intent Classification
Charles Costello, Ruixi Lin, Vishwas Mruthyunjaya, Bettina Bolla, Charles Jankowski
arXiv:1806.07914 [cs.CL]

* we ensemble both different model initializations and different model archits
  in multilingual and multi-domain contexts
* a new banking domain dataset
* we compare results against the standard ATIS dataset and the Chinese SMP2017
* conclusion: ensembling provides significant performance increases, and
  multi-layer ensembling is a no-risk way to improve performance on intent clas
  * a diverse ensemble of simple models can perform comparable to much more
    sophisticated SOTA models
  * Our best F 1 scores on ATIS, Banking, and SMP are 97.54%, 91.79%, & 93.55%
    * compare well with the SOTA on ATIS and best submission to the SMP2017
      competition. The total ensembling performance increases we achieve are
      0.23%, 1.96%, and 4.04% F 1 respectively
