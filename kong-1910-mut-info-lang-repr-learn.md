A Mutual Information Maximization Perspective of Language Representation Learnng
L Kong, C de Masson d'Autume, W Ling, L Yu, Z Dai, D Yogatama
arXiv:1910.08350

# Abstract

We show SOTA word representation learning methods maximize an objective function
that is a lower bound on the mutual information between different parts of a
word sequence (i.e., a sentence)
* Our formulation unifies classical word embedding models (e.g., Skip-gram) and
  modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our
* can be used to construct new self-supervised tasks
  * example by drawing inspirations from related methods based on mutual
    information maximization that have been successful in computer vision, and
  * a simple self-supervised objective that maximizes the mutual information
    between a global sentence representation and n-grams in the sentence
* Our analysis offers a holistic view of representation learning methods to
  transfer knowledge and translate progress across multiple domains (e.g.,
  natural language processing, computer vision, audio processing)
