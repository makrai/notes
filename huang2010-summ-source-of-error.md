What Have We Achieved on Text Summarization?
Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, Yue Zhang
Accepted by EMNLP 2020 arXiv:2010.04529 [cs.CL]

# Abstract

* to gain more understanding of summarization on a fine-grained synt and sem, we
  * consult the Multidimensional Quality Metric (MQM) and 
  * quantify 8 major sources of errors on 10 representative models manually.
    * under similar settings, extractive summarizers are in general better than
      their abstractive counterparts thanks to strength in faithfulness and
      factual-consistency;
    * milestone techniques such as copy, coverage and hybrid
      extractive/abstractive methods do bring specific improvements but also
      demonstrate limitations;
    * pre-training techniques, and in particular sequence-to-sequence
      pre-training, are highly effective for improving text summarization, with
      BART giving the best results. 

# 7 Conclusion 9
