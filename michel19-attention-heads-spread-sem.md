Are Sixteen Heads Really Better than One?  
NeurIPS 2019 
Paul Michel, Omer Levy, Graham Neubig

# Abstract

* a large proportion of attention heads can be removed at test time without
  significantly impacting performance, and
* some layers can even be reduced to a single head
* analysis on machine translation models reveals that the 
  * self-attention layers can be significantly pruned, while the 
  * encoder-decoder layers are more dependent on multi-headedness
