Pre-Training with Whole Word Masking for Chinese BERT
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu
arXiv:1906.08101 [cs.CL]

https://github.com/ymcui/Chinese-BERT-wwm

# Abstract

* version of BERT has been released with Whole Word Masking (WWM), which
* we adapt whole word masking in Chinese text, that masking the whole word
* verified on various NLP tasks, across sentence-level to document-level,
  * machine reading comprehension (CMRC 2018, DRCD, CJRC)
  * natural language inference (XNLI)
  * sentiment classification (ChnSentiCorp)
  * sentence pair matching (LCQMC, BQ Corpus)
  * document classification (THUCNews).
* significant gain. Moreover,
* we also examine the effectiveness of the Chinese pre-trained models:
  BERT, ERNIE, BERT-wwm, BERT-wwm-ext, RoBERTa-wwm-ext, and
  RoBERTa-wwm-ext-large
