A generalizable speech emotion recognition model reveals depression and remission
Lasse Hansen, Yan-Ping Zhang, Detler Wolf, K Sechidis, N Ladegaard, R Fusaroli
Acta Psychiatrica Scandinavica https://doi.org/10.1111/acps.13388

# Abstract

* automated voice analyses for affective disorders suffer from small sample
  * ? generalizability on external data. We investigated a generalizable
  * we transfer learning: We
    * train machine learning models on easily accessible non-clinical datasets
    * test them on novel clinical data in a different language.
* Methods
  * A Mixture of Experts machine learning model was trained to infer happy/sad
    * using three publicly available emotional speech corpora
      in German and US English. We examined the model's predictive ability to
  * test on Danish speaking patient
    * healthy controls (N = 42), patients with
    * first-episode major depressive disorder (MDD) (N = 40), and the
    * subset of the same patients who entered remission (N = 25) based on
    * evaluated on raw, de-noised, and speaker-diarized data.
* Results
  * separation between healthy controls and depressed patients at the first
    visit, obtaining an AUC of 0.71. Further, speech from patients in
  * remission was indistinguishable from that of the control group. Model
  * predictions were stable throughout the interview, suggesting that
  * 20â€“30 s of speech might be enough to accurately screen a patient.
  * Background noise (but not speaker diarization) heavily impacted
* Limitations: No data from non-remitters, meaning that changes to voice for
