BERT: Pre-training of Deep Bidirectional Transformers for Language Understand
Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
arXiv:1810.04805 [cs.CL]

# Abstract

* BERT stands for Bidirectional Encoder Representations from Transformers
* pre-train deep bidirectional representations from unlabeled text
  by jointly conditioning on both left and right context in all layers. As a
* fine-tuned with just one additional output layer to create SOTA models for a
  * without substantial task-specific architecture modifications.  BERT is
  * conceptually simple and empirically powerful. It obtains
  * new SOTA results on eleven natural language processing tasks, including
    * GLUE score to 80.5% (7.7% point absolute improvement),
    * MultiNLI accuracy to 86.7% (4.6% absolute improvement),
    * SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute)
    * SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)
