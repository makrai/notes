Recurrent neural network language model adaptation with curriculum learning
December 2014 Computer Speech & Language 33(1)
Yangyang Shi, Martha Larson, Catholijn M Jonker

# Abstract

* language model adaptation for recurrent neural network language models
  * SOTA method for language modeling in the area of speech recognition
* Curriculum learning: a well-planned ordering of the training data, during the
* RNNLMs ... can exploit word-dependency information over arbitrarily long dist
* In this paper, we
  * within-domain adaptation and limited-data within-domain adaptation
  * curricula that start with general data: characterizing the whole domain
    * move towards specific data, ie, characterizing the sub-domain targeted
  * result in a[n] interpolation between general data and sub-domain-specific
