ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
Zhenzhong Lan, Mingda Chen, S Goodman, K Gimpel, Piyush Sharma, Radu Soricut
arXiv:1909.11942 [cs.CL]

# Abstract

* we present two parameter-reduction techniques [for] memory and training speed
* Comprehensive empirical evidence: models scale much better 
  compared to the original BERT
* a self-supervised loss that focuses on modeling inter-sentence coherence, and
* consistently helps downstream tasks with multi-sentence inputs.  As a result,
  our best model establishes new state-of-the-art results on the GLUE, RACE,
  and SQuAD benchmarks while having fewer parameters compared to BERT-large.
  The code and the pretrained models are available at this https URL. 
