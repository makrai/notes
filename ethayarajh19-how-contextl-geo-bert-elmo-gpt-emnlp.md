Kawin Ethayarajh
How Contextual are ...? Comparing the Geometry of BERT, ELMo, and GPT-2
EMNLP 2019

#Abstract

* Are there infinitely many context-specific representations for each word, 
  * or are words essentially assigned one of a finite number of word-sense[s]
* In all layers of ELMo, BERT, and GPT-2, on average, 
  less than 5% of the variance in a word's contextualized representations can
  be explained by a static embedding
