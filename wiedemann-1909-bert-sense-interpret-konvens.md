Does BERT Make Any Sense? Interpretable WSD with Contextualized Embeddings
Gregor Wiedemann, Steffen Remus, Avi Chawla, Chris Biemann
Konferenz zur Verarbeitung nat√ºrlicher Sprache (KONVENS) 2019 arXiv:1909.10430

# Abstract

* We
  * using a nearest neighbor classification on CWEs
  * compare the performance of different CWE models for the task and can report
    improvements above the current SOTA for two standard WSD benchmark datasets
  * show that the
    * pre-trained BERT model can place polysemic words into 'sense' region
    * ELMo and Flair NLP do not seem to possess this ability
