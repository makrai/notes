GO FIGURE: A Meta Evaluation of Factuality in Summarization
Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, Jianfeng Gao
ACL 2021 Findings arXiv:2010.12834 [cs.CL]

# Abstract

* automatic metrics for evaluating the factuality of machine text
  * a new line of research
* we introduce GO FIGURE, a meta-evaluation framework
  for evaluating factuality evaluation metrics
* five necessary and intuitive conditions to evaluate factuality metrics
  on diagnostic factuality data across three summarization tasks
* our meta-evaluation framework provides a robust and efficient evaluation that
  * extensible to multiple types of factual consistency and standard generation
    metrics, including QA metrics
* QA metrics
  * generally improve over standard metrics that measure factualt across domains
  * performance is highly dependent on the way in which questions are generated

# 1 Intro

* semantically constrained text generation
  * goal: fluent, coherent, relevant, as well as factually correct
  * neural approaches have shown tremendous improvements in this direction
    (Liu and Lapata, 2019; Guo+ 2018; Durmus+ 2020; Wang+ 2020)
    * Han Guo, Ramakanth Pasunuru, and Mohit Bansal
      Soft layer-specific multi-task summ with entailment and question gen
      ACL 2018
  * issue: factually inconsistent text
    * somewhat distorted or fabricated facts about the source text
    * abstract models generate text with up to 30% factual inconsistencies
      (Kryscinski+ 2019; Falke+ 2019; Zhu+ 2020)
      * T Falke, LFR Ribeiro, Prasetya A Utama, Ido Dagan, Iryna Gurevych
        Ranking generated summaries by correctness: An interesting but
        challenging application for natural language inference
        ACL 2019a=2019b
      * Chenguang Zhu, William Hinthorn, Ruochen Xu, Qingkai Zeng, Michael Zeng,
          Xuedong Huang, and Meng Jiang
        Boosting factual correctness of abstractive summarization
        ArXiv, abs/2003.08612
* Commonly used metrics
  * fail to capture structural aspects of language like negation
  * poorly correlate with human judgements
    (Hashimoto+ 2019; Clark+ 2019; Sellam+ 2020)
    * T. Hashimoto, Hugh Zhang, and Percy Liang. 2019
      Unifying human and statistical evaluation for natural language gen
      ArXiv, abs/1904.02792
* we propose GO FIGURE, a meta-evaluation framework
  * General Outline for Factuality In Generative UndeRstanding Evaluation
  * suited for multiple domains
    * extreme summarization, multi-sentence news summarization and the
      understudied dialogue summarization domain
* contributions
  * a set of diagnostics for measuring sensitivity of metrics to factual incons
  * a diagnostic evaluation dataset of context/summary pairs
    for measuring effectiveness of new factuality metrics in a controlled settin
  * an evaluation dataset of summaries generated by transformer-based models
    (Raffel+ 2019 T5) annotated with types of factual errors (see A.3)

# 2 Factuality Metric Meta Evaluation

* reference summaries may be an incomplete repr of the salient facts in a doc
* we consider factuality in terms of how well candidate summaries are factually
  grounded with respect to the source document
* five conditions for a factual consistency metric M (D, S i ) to measure
  * Boundedness (I)
    There exists S r , S f such that M (D, S r ) ≤ M (D, S i ) ≤ M (S f )
    * provide points of comparison
  * Sensitivity (II) The metric should correlate with factuality of `S_i`
    * i.e. with the number of injected errors
  * Robustness (III) across types of factual errors
  * Generality (IV), satisfy conditions I, II, III and V across domains
    * Reiter and Belz (2009) highlight the importance of this
  * Human Correlation (V), correlation with human judgements (Chaganty+ 2018)
    * Arun Chaganty, Stephen Mussmann, and Percy Liang
      The price of debiasing automatic metrics in natural language evalaution
      ACL 2018

## 2.1 Testing Factuality Metric Validity

* our controlled experiments use transformed versions of the reference summary
  with injected errors
  factually consistent as a transformed summary
* sensitivity (Condition II): Pearson’s r between the factual inconsistency
  level of the summaries (i.e. the number of injected errors) and the average
  metric score. Then we measure statistical significance using the p-value from
  a two-tailed hypothesis test
* robustness and generality (Conditions III and IV)
  * domains and the factual error types shown in Figure 1
  * extrinsic entity refers to entities that did not previously appear in the
    source, while an intrinsic entity appeared in the source

## 2.2 Theoretical Cases

* For non-deterministic metrics, restrictions on variability between runs may
  also be desired

# 3 Evaluation Datasets

* three datasets:
  * 1-sentence BBC news summaries from the XSUM extreme dataset (Narayan+ 2018)
  * multi-sentence summaries from theCNN/DailyMail dataset (Nallapati+ 2016)
  * the recently released SAMSUM corpus (Gliwa+ 2019)
    * English language conversations written by linguists and aligned
      multisentence summaries

## 3.1 Diagnostic Datasets

* transformed reference summaries with simulated factuality errors
  * i.e. controlled setting
* summaries generated by SOTA transformer summarization models that allows us to
  * in a real data setting. We sample 500 source / summary pairs for each domain

### 3.1.1 Model-Generated Datasets

* summs from fine-tuned T5 encoder-decoder summarization models (Raffel+ 2019)
  * T5 was pretrained on news summarization data
  * we use either beam search or sample-based decoding strategies
  * We then annotate the generated summaries for fine-grained factual error typs

# 4 Factuality Metrics for Evaluation

* recently proposed metrics which use two types of proxy NLU objectives
  aimed at implicitly capturing factuality in generated text
* see the appendix for details of metrics
* Datasets by proxy:
  * question-answering (QA)
    * SummaQA (which uses QA pairs from the source, Scialom+ 2019) and
    * FEQA (which uses QA pairs from the summary, Durmus+ 2020), while
  * a masked token prediction cloze task
    * BLANC-Help and BLANC-Tune (Vasilyev+ 2020)
  * We also measure the factual-awareness of
    * BERTScore (Zhang+ 2020),
      * a summarization metric that is aimed primarily at improving coherency
        rather than factual consistency, and
    * standard summarization evaluation metrics (eg ROUGE (Lin, 2004))

# 5 Meta-Analysis of Factuality Metrics

## 5.1 Controlled Data Experiments

* QA metrics, ROUGE-(2/3) and BERTScore generally perform well
* ROUGE(1/L) are frequently invalid as factuality metrics (Tables 2 and 3),
* Cloze metrics varies across domains
  (BLANC-Tune is invalid on XSUM, but does fairly well on other domains)
* performance of metrics tends to be
  much lower on news domains when we consider non-entity-based errors
  * with the exception of QAbased metrics, ROUGE-(2/3) and BERTScore,
  * indicating that factuality and standard metrics
    * fairly attuned to changes in factual consistency that relate to
      entity-based errors
    * hE less robust to other types of factual errors

## 5.2 Comparison with Human Evaluation of Model Generations

* metrics displaying invalid behavior on controlled data also do so in model gen
  * i.e. meta-evaluation with controlled data is effective as a diagnostic tool
  * non-entity errors are difficult for standard summarization metrics
    * frequently produced by abstractive summ models
* better in controlled ?=> better in model gen (both evaled by hum)
  * XSUM
    * generally yes (SummaQA, ROUGE-(2/3) and BERTScore)
    * with the exception of FEQA
* strong overall performance of ROUGE-3
  * consistent with the findings of Fabbri+ (2021) on CNNDM,
  * our work confirms that this metric is more consistently correlated with
    factuality than other ROUGE variations across domains

# 6 Related Work

* evaluation of automatic metrics and human evaluation for NLG systems
  * mainly focused on general analysis of output quality or coherence and fluenc
    (Callison-Burch+ 2007; Graham, 2015; Fabbri+ 2021), rather than factuality
* factual errors and hallucinations in the output of neural summarization models
  (Cao+ 2018; Massarelli+ 2019; Zhao+ 2020; Falke+ 2019; Goodrich+ 2019;
  Celikyilmaz+ 2020)
  * Ziqiang Cao, Furu Wei, W. Li, and Sujian Li
    Faithful to the original: Fact aware neural abstractive summarization
    2018 AAAI
  * Luca Massarelli, F Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel,
      Vassilis Plachouras, F Silvestri, S Riedel
    How decoding strategies affect the verifiability of generated text
    ArXiv, abs/1911.03587
  * Z. Zhao, Shay B. Cohen, and B. Webber
    Reducing quantity hallucinations in abstractive summarization
    ArXiv, abs/2009.13312
  * B. Goodrich, V. Rao, Mohammad Saleh, and Peter J.  Liu
    Assessing the factual accuracy of generated text
    ACM SIGKDD 2019 International Conference on Knowledge Discovery & Data Minin
  * Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao
    Evaluation of text generation: A survey
    ArXiv, abs/2006.14799
* QA and cloze task objectives for eval factuality on specific domains
  (Eyal+ 2019; Huang+ 2020). We aim to evaluate these metrics more broadly

## 6.1 Discussion of Meta Evaluation and Conclusion

* Standard summarization metrics are not always valid measures of factuality
  * ROUGE-1 and ROUGE-L fail to accurately measure factual inconsistency across
    domains in our controlled analysis
  * ROUGE-L results raise the question of context relevance
    * While ROUGE-L takes into account more context than other ROUGE variations,
      this context may not be relevant for assessing factuality
  * prior work has also noted that ROUGE-N outperforms ROUGE-L
    (Rankel+ 2013; Fabbri+ 2021)
* Analysis on human annotated data is still necessary as an upper-bound on meta
  * BLANC-Help, FEQA metric and BERTScore values
    decrease with factual inconsistency on controlled data, the metrics may
    sometimes be positively correlated with factual inconsistency on generated
    * emphasizes the importance of a expert curated test set
      as part of the GO FIGURE meta evaluation for the most rigorous testing
* question-answering objective is promising for measuring factual consistency
  across domains, but effectiveness depends on the question
  * dependent on the way in which questions are asked
  * both QA metrics use SQuAD-based systems (Rajpurkar+ 2016), asking questions
    from the source rather than the summary is most robust across domains
  * metrics based on more contextual QA like commonsense (Shwartz+ 2020)
  * Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi
    Unsupervised commonsense question answering with selftalk
    EMNLP 2020
* summary meta-metric results (eg correlation on simulated data) could be used
  as rewards for reinforcement learning driven approaches to training factuality
  metrics

# Appendix

## A.2 Evaluation Metric Details

## QA-Based Quality Score. Given a source or reference document D and candidate

* SummaQA (Scialom+ 2019)
  * questions are generated from the source document D
* FEQA (Durmus+ 2020) metrics. For the SummaQA
  * FEQA generates questions from S i and uses D to answer these questions
* The generation quality score is
  * typically the aggregated F 1 score measuring the similarity between
    ground-truth answers for questions generated from D and the answers
  * SummaQA also generally includes the aggregated model confidence
    probabilities for predictions

## Masked LM Prediction (Cloze Task) Score

* measuring the ability of a NLU system to accurately predict masked tokens in
  the source document, given access to the information in S i
* We use two variants of BLANC (Vasilyev+ 2020), BLANC-Help and BLANC-Tune
  * BLANC-Help uses both D and S i as input to a pretrained masked token
    prediction model, while
  * BLANC-Tune only uses D as input to a model that has been finetuned on the
    candidate summary
  * Both metrics are aimed at capturing fluency, infoness and factual correctnss

## Semantic Similarity. Semantic similarity metrics measure the overlap

* between contextual embeddings of a source or reference document D and
  candidate summary S i
* We use BERTScore (Zhang+ 2020)
  * correlate better with human judgements of coherency than standard metrics
    and similarly to n-gram metrics on factual consistency of CNNDM summaries
    (Wang+ 2020)

## Lexical Overlap. Finally, we test ROUGE (Lin, 2004), which is the standard

* ROUGE-1 and ROUGE-2, as well as ROUGE-L, which measures longest common
* We follow prior work that considered ROUGE in factual consistency evaluations
  (Wang+ 2020), though it has also been previously noted that ROUGE can
  underweight good summarization examples (Novikova+ 2017)

## A.3 Simulated Data Transformations

* We inject errors into reference summaries
* first using a part-of-speech tagging model and NER (spaCy)
* All datasets are comprised of English language articles or dialogues and
  summaries, and we use the spaCy English NLP models
* Intrinsic entity errors. To inject intrinsic entity errors into a summary S,
  * We then swap a random entity in the reference summary for a different entity
    of the same label type in the doc
* Extrinsic entity errors: substitute entity from the whole corpus of docs
* To swap an adjective for its antonym, we use WordNet (Miller, 1995) to obtain
* Pronoun entity errors with a preset list of commonly used pronouns
  * eg we swap she/her with he/him
* Verb Negation. We use a rule-based system for verb negation
  * based on verb tense, and predict tense based on the suffix and preceding
    words
* varying effects depending on the average length of summaries for a corpus
  * summ length varies by corpus
  * future work may explore length-controlled error injection
