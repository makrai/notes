K-EmoCon, a multimodal sensor dataset
  for continuous emotion recognition in naturalistic conversations
Cheul Young Park, Narae Cha, Soowon Kang, Auk Kim, Ahsan Habib Khandoker,
  Leontios Hadjileontiadis, Alice Oh, Yong Jeong, & Uichin Lee
Scientific Data volume 7, Article number: 293 (2020)

# Abstract

* Recognizing emotions during social interactions
* low-cost mobile sensors
* lack of naturalistic affective interaction data
  * Most existing emotion datasets were collected in constrained environments
* K-EmoCon is a multimodal dataset
  * comprehensive annotations of continuous emotions during natural conversat
  * multimodal measurements, including
    * audiovisual recordings, EEG, and peripheral physiological signals,
      acquired with off-the-shelf devices from
      16 sessions of approximately 10-minute long paired debates on a soc issue
* emotion annotations from all three available perspectives:
  self, debate partner, and external observers
* annotated at intervals of every 5 seconds while viewing the debate footage,
  in terms of arousal-valence and 18 additional categorical emotions
* the first publicly available emotion dataset accommodating the
  multiperspective assessment of emotions during social interactions

# 1 Intro

* facial expressions are compound^14, relative^15, and misleading^16
  * facial expressions lack reliability, specificity, and generalizability^17,
  * contextual dependency^18,19,20 and individual variability of emotions^21,22
* K-EmoCon, a multimodal dataset acquired from
  * 32 subjects participating in 16 paired debates on a social issue
  * physiological sensor data collected with 3 off-the-shelf wearable devices,
    audiovisual footage of participants during the debate, and
    continuous emotion annotations
* the first dataset with emotion annotations from all possible perspectives:
  subject him/herself, debate partner, and external observers
