Improving Sentence Representations via Component Focusing
Xiaoya Yin 1,*, Wu Zhang 1, Wenhao Zhu 1,*, Shuang Liu 1 and Tengjun Yao 2
Appl. Sci. 2020, 10(3), 958; https://doi.org/10.3390/app10030958 

# Abstract

* bidirectional encoder representations from transformers (BERT) has attracted
* different sentence components serve diverse roles in the meaning of a sent
  * subject, predicate, and object serve the most crucial roles as they
* [our] representation consists of a 
  * basic part which refers to the complete sentence, and a 
  * component-enhanced part: subject, predicate, object, and their relations.
* evaluate [on] semantic textual similarity and entailment classification.
  * significant performance gain compared to other methods.
