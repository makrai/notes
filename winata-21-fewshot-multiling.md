Language Models are Few-shot Multilingual Learners
GI Winata, A Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, Pascale Fung
EMNLP | MRL 2021

# Abstract

* General-purpose language models have demonstrated impressive capabilities,
  performing on par with SOTA approaches on a range of downstream NLP tasks and
  benchmarks when inferring instructions from very few examples
* we evaluate the multilingual skills of the GPT and T5 models in conducting
  * multi-class classification on non-English languages without any parameter
    updates
  * given a few English examples as context, pre-trained language models can
    predict not only English test samples but also non-English ones. Finally,
  * in-context few-shot cross-lingual prediction results of language models are
    * significantly better than random prediction, and they are
    * competitive compared to the existing SOTA cross-lingual models and
      translation models
