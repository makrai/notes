Comparative Analyses of BERT, RoBERTa, DistilBERT, and XLNet
  for Text-based Emotion Recognition
Francisca Adoma Acheampong, Henry Nunoo-Mensah, and Chen Wenyu
2020 17th International Computer Conference on Wavelet Active Media â€¦

# Abstract

* The models are fine-tuned on the ISEAR data to distinguish emotions into
  anger, disgust, sadness, fear, joy, shame, and guilt
  | RoBERTa	| XLNet	  | BERT	  | DistilBERT	|
  | 0.7431	| 0.7299	| 0.7009	| 0.6693	    |

# Introduction

* models
  * Bidirectional Encoder Representations from Transformers (BERT) pre-trained
  * Generalized Auto-regression Pre-training for Language Understanding (XLNet),
  * Robustly optimized BERT pre-training Approach (RoBERTa), and
  * DistilBERT pre-trained models were necessary proposals for mitigating
* International Survey on Emotion Antecedents and Reactions (ISEAR) dataset

# II related works

# III the emotion detection pipeline and the model implementation

# IV experiments

# V results

# VI conclusion and future works
