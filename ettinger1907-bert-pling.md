What BERT is not: Lessons from a new suite of psycholinguistic diagnostics LM
Allyson Ettinger
arXiv:1907.13528

# Abstract

* we introduce a suite of diagnostics drawn from human language experiments,
  which allow us to ask targeted questions about the information used by
  language models for generating predictions in context. As a case study, we
* BERT can generally distinguish good from bad completions involving shared
  category or role reversal, albeit with less sensitivity than humans, and it
  * robustly retrieves noun hypernyms, but it 
  * struggles with challenging inferences and role-based event prediction -- and
  * clear insensitivity to the contextual impacts of negation. 
