Toy Models of Superposition
N Elhage, T Hume, C Olsson, N Schiefer, T Henighan, S Kravec, Z Hatfield-Dodds,
  R Lasenby, D Drain, C Chen, R Grosse, S McCandlish, J Kaplan, D Amodei,
  M Wattenberg, C Olah
arXiv:2209.10652  https://transformer-circuits.pub/2022/toy_model/index.html

* Neural networks often pack many unrelated concepts into a single neuron
  * a puzzling phenomenon known as 'polysemanticity'
  * makes interpretability much more challenging
* we provide a toy model where polysemanticity can be fully understood
  * polysemant <~ models storing additional sparse features in superposition
* We demonstrate the existence of a phase change,
  * a surprising connection to the geometry of uniform polytopes, and
  * evidence of a link to adversarial examples
  * potential implications for mechanistic interpretability
