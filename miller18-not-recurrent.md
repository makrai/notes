* truncated backpropagation [through] time cannot learn patterns significantly
  longer than k steps
* We show if the recurrent model is stable (meaning the gradients can not
  explode), then the model can be well-approximated by a feed-forward network
* not all models trained in practice are stable. We also give empirical
* the stability condition can be imposed on certain recurrent models without
  loss in performance
