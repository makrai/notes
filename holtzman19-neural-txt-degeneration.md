The Curious Case of Neural Text Degeneration
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi
ICLR 2020 arXiv:1904.09751 [cs.CL]

* counter-intuitive empirical observation
  * the use of likelihood as training objective leads to high quality models for
    a broad range of language understanding tasks,
  * using likelihood as a decoding objective leads to text that is bland and
    strangely repetitive
* we reveal
  * surprising distributional differences between human and machine text
  * decoding strategies alone can dramatically effect the quality
  * Nucleus Sampling i.e. `top_p`, a simple but effective method

# 1 Intro

* top-k sampling that samples the next word from the top k most probable choics
  (Fan+ 2018; Holtzman+ 2018; Radford+ 2019),
  * instead of aiming to decode text that maximizes likelihood
  * e.g. beam search, lead to text that is incredibly degenerate, even when
    using SOTA models such as GPT-2 Large, as shown in Figure 1.  This may seem
  * the highest scores for longer texts are often generic, repetitive, & awkward
  * Figure 2 exposes how different the distribution of probabilities
* pure sampling — sampling directly from the probabilities predicted by the
  model — results in text that is incoherent and almost unrelated to the context
* we compare various distri properties of generated text to the ref distribution
  * e.g. likelihood of veering into repetition and the perplexity of gened txt
  * perplexity reveals that text generated by (4.2)
    * maximization or top-k sampling is too probable, indicating a
    * pure sampling produces text that is significantly less likely than the
      gold, corresponding to lower generation quality
  * Vocabulary usage and Self-BLEU (Zhu+ 2018) statistics reveal that (5.2)
    * high values of k are needed to make top-k sampling match human statistics
      * variance in likelihood, hinting at qualitatively observable incoherency
    * Nucleus Sampling can easily match reference perplexity
      through tuning the value of p, avoiding the
      incoherence caused by setting k high enough to match distributional stats
* we perform Human Unified with Statistical Evaluation (6.1)
  (HUSE; Hashimoto+ 2019)
  * jointly assess the overall quality and diversity of the decoding strategies,
  * Nucleus Sampling is the best overall decoding strategy
* We include generated examples for qualitative analysis – see Figure 3 for a

# 2 Background

## 2.1 Text generation decoding strategies

* disadvantages of generation by maximization: high grammaticality but low diver
  (Kulikov+ 2019; Holtzman+ 2018; Fan+ 2018)
* Generative Adversarial Networks (GANs) are prominent dirc (Yu+ 2017; Xu+ 2018)
  * hE, when quality and diversity are considered jointly,
    GAN-generated text fails to outperform generations from language models
    (Caccia+ 2018; Tevet+ 2019; Semeniuta+ 2018).  Work on
* neural dialog systems have proposed methods for diverse beam search, using a
  * task-specific diversity scoring function or
  * constraining beam hypotheses to be sufficiently different
  * (Li+ 2016a; Vijayakumar+ 2018; Kulikov+ 2019; Pal+ 2006). While such utility
  * not remove the need to choose an appropriate decoding strategy, and we
  * Nucleus Sampling will have complementary advantages in such approaches
* “unlikelihood loss” (Welleck+ 2020), which
  * decreases training loss on repeated tokens and thus
    implicitly reduces gradients on frequent tokens as well. Our focus is on
* future work will likely combine training-time and inference-time solutions

## 2.2 Open-ended vs directed generation

* directed generation: tasks defined through (input, output) pairs, such that
  * the output is a constrained transformation of the input. Example
  * applications include machine translation (Bahdanau+ 2015),
    data-to-text generation (Wiseman+ 2017), and
    summarization (Nallapati+ 2016)
  * Typical archit
    * encoder-decoder architectures with attention (Bahdanau+ 2015; Luong+ 2015)
    * attention-based architectures such as the Transformer (Vaswani+ 2017)
  * Generation is usually performed using beam search;
    * since output is tightly scoped by the input, repetition and genericness
      are not as problematic
    * hE, similar issues have been reported when using
      * large beam sizes (Koehn & Knowles, 2017) and more recently with
      * exact inference (Stahlberg & Byrne, 2019)
* Open-ended generation, which
  * e.g. conditional story generation and contextual text continuation
  * significant advances in neural language models
    (Clark+ 2018; Holtzman+ 2018; Fan+ 2018; Peng+ 2018; Radford+ 2019)
  * While the input context restricts the space of acceptable output gens,
    considerable degree of freedom in what can plausibly come next,
    unlike in directed generation settings
  * Our work addresses the challenges faced by this increased level of freedom,
* some tasks, e.g. goal-oriented dialog, may fall somewhere in between open/dir

# 3 Language model decoding

* finding the optimum argmax sequence from RNNs or Transformers is not tractable
  (Chen+ 2018)
* common practice is to use beam search (Li+ 2016b; Shen+ 2017; Wiseman+ 2017)
  hE, it does not lead to high quality text (Fan+ 2018; Holtzman+ 2018)

## 3.3 SAMPLING WITH TEMPERATURE

* shape a probability distribution through temperature (Ackley+ 1985)
  * applied widely to text generation (Ficler & Goldberg, 2017; Fan+ 2018;
    Caccia+ 2018). Given the logits u 1:|V | and temperature t, the softmax is
* Setting t ∈ [0, 1) skews the distribution towards high probability events,
  * has also been used to partially alleviate the issues of top-k sampling
    by shaping the distribution before top-k sampling (Radford+ 2018; Fan+ 2018)
* while lowering the temperature improves generation quality, it comes at the
  cost of decreasing diversity (Caccia+ 2018; Hashimoto+ 2019)

# 4 Likelihood evaluation

## 4.3 NATURAL LANGUAGE DOES NOT MAXIMIZE PROBABILITY

* the per-token probability of natural text is, on average, much lower than text
  generated by beam search. Natural language rarely remains in a high
* Language models that assign probabilities one word at a time without a global
  model of the text will have trouble capturing this effect. Grice’s Maxims of
  Communication (Grice, 1975) show that people optimize against stating the
  obvious. Thus, making every word as predictable as possible will be
  disfavored

# 5 Distributional statistical evaluation

## 5.1 Zipf distribution analysis

* perfect exponential curve, where s = 1 (Piantadosi, 2014)
  * Figure 7: vocabulary distributions along with estimated Zipf coefficients
  * Steven T Piantadosi
    Zipfs word frequency law in natural language: A critical review and future
    Psychonomic bulletin & review, 21(5):1112–1130, 2014
* pure sampling is the closest to the human distribution,
  * followed by Nucleus Sampling. The visualization of the distribution shows
  * pure sampling slightly overestimates the use of rare words, likely
    * one reason why pure sampling also has higher perplexity than human text
  * lower temperature sampling avoids sampling these rare words from the tail,

## 5.2 Self-BLEU

* Self-BLEU (Zhu+ 2018) as a metric of diversity
* calculated by computing the BLEU score of each generated document using all
  other generations in the evaluation set as references. Due to the expense of
* we sample 1000 generations, each of which is compared with all 4999 other
* results largely follow that of the Zipfian distribution analysis as a
  * high values of k and t are needed to get close to the reference
    distribution, though these result in unnaturally high perplexity (§4)

## 5.3 Repetition

* Nucleus Sampling and top-k sampling have the least repetition
* temperature sampling has more repetition
  * unless very high temperatures are used, which we have shown negatively
    affects coherence (as measured by high perplexity)
* all stochastic methods face repetition issues when their tuning parameters are
  set too low, which tends to overtruncate, mimicking greedy search. Therefore
* only Nucleus Sampling satisfies all the distributional criteria

# 6 Human evaluation

## 6.1 Human unified with statistical evaluation (huse)

* pure human evaluation does not take into account diversity
* HUSE (Hashimoto+ 2019) to combine human and statistical evaluation. HUSE is
  * discriminator to distinguish between text drawn from the human and model dis
  * based on only two features: The probability assigned by the language model,
    and human judgements of typicality of generations
* Text that is close to the human distribution in terms of quality and diversity
  should perform well on both likelihood evaluation and human judgements
* Initial exploration of applying HUSE directly led to top-k and Nucleus
  Sampling receiving scores of nearly 0 due to truncation, despite humans
* As a proxy, when generating the text used to compute HUSE, we interpolate
  (with mass 0.1) the original probability distribution with the top-k and Nucle

## 6.2 Qualitative analysis

* example generations. Unsurprisingly,
  * beam search gets stuck in a repetition loop it cannot escape. Of the
  * full sampling is clearly the hardest to understand, even inventing a new
    word “umidauda”, apparently a species of bird. The generation produced by
  * Nucleus Sampling isn’t perfect – the model appears to confuse whales with
    birds, and begins writing about those instead. Yet,
  * top-k sampling immediately veers off into an unrelated event. When
  * top-k sampling is combined with a temperature of 0.7, as is commonly done
    (Radford+ 2019; Fan+ 2018), the output devolves into repetition
