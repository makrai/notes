Kawin Ethayarajh
uSIF: Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Basline
Rep4NLP@ACL 2018

unsupervised smoothed inverse frequency (uSIF)

# Abstract

* SIF [outperforms] LSTMs on textual similarity tasks
* we
  * show that word vector length has a confounding effect
    on the probability of a sentence ... in Arora+ ’s model
  * propose a random walk model that is robust to this confound
  * probability of word ... is inversely related
    to the angular distance between the word and sentence embeddings
  * beats Arora+ ’s by up to 44.4% on textual sim & competitive with SOTA
  * no hyperparameter tuning [as opposed? to `a` in SIF]

# 1 Intro

* methods ranging from a weighted average of word embeddings to
  convolutional, recursive, and recurrent neural networks
  (Le and Mikolov, 2014; Kiros+ 2015; Luong+ 2013; Tai+ 2015)
* sophisticated architectures are often outperformed,
  by sentence embeddings generated as a simple average of tuned word embeddings
  particularly in transfer learning settings, (Wieting+ 2016b)
* Arora+ (2017) provided a more powerful approach
  * ‘weakly supervised’, since it requires hyperparameter tuning (Cer+ 2017)
* we first propose a class of worst-case scenarios for Arora+ ’s (2017) rand wk
  * sentence g that is dominated by words with zero similarity, and some
  * sentence h that is dominated by identical words, long vec
* contributions in this paper are three-fold.  First,
  * [new] random walk model that is robust to distortion by vector length,
  * probability of a word vector being generated by a discourse vector is
    inversely related to the angular distance between them
  * MAP estimate for the sentence embedding as follows:
    * normalize the word vectors, take a
    * weighted average of them, a / (p(x) + 1/2 a)
    * subtract ... the projection on their first m principal components. We call
  * outperforms Arora+ ’s by up to 44.4% on textual similarity tasks, and is
    * competitive with SOTA methods

## 3.3 An Angular Distance–Based Random Walk Model

* m common discourse vectors ... represent various types of frequent discourse,
  such as stopwords
* differences [from the SIF weighting scheme (Arora+ 2017)]
  * model that is robust to the confounding effect of word vector length
  * in SIF, `a` is a hyperparameter that needs to be tuned
    * in our approach, we can calculate `a` directly as a function of the
      vocabulary V and the number of words in the sentence, |s|.
    * a = .0012

## 4.2 Experimental Setting

* unigram probability distribution used by [Arora], based on the enwiki dataset
  (Wikipedia, 3B words). Our preprocessing of the sentences is limited to
* three types of word vectors:
  * GloVe vectors (Pennington+ 2014)
  * PARAGRAM-SL999 (PSL) vectors (Wieting+ 2015), tuned on the SimLex999
  * ParaNMT-50 vectors (Wieting and Gimpel, 2017a), tuned on 51M
    English-English sentence pairs translated from English-Czech sentence pairs
* n ~ 11 and was estimated using sentences from all corpora. The value of
* a = 1.2 * 10^3 . Our results are denoted as X+UP, where X 2 {‘GloVe’, ‘PSL’,
  ‘ParaNMT’}, U denotes uSIF-weighting, and P denotes piecewise common
  component removal.
