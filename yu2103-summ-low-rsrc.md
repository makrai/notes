AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization
Tiezheng Yu, Zihan Liu, Pascale Fung
NAACL 2021, 2103.11332 [cs.CL]

# Abstract

* six diverse target domains in a low-resource setting. Specifically, we
  investigate the second phase of pre-training on large-scale generative models
  under 
* three settings: 
  * source domain pre-training; 
  * domain-adaptive pre-training; and 
  * task-adaptive pre-training. Experiments show that the effectiveness of
    pre-training is correlated with the similarity between the pre-training data
    and the target domain task. Moreover, we find that continuing pre-training
    could lead to the pre-trained model's catastrophic forgetting, and a
    learning method with less forgetting can alleviate this issue. Furthermore,
    results illustrate that a huge gap still exists between the low-resource and
    high-resource settings, which highlights the need for more advanced domain
    adaptation methods for the abstractive summarization task. 
