As Good as New: ... Recycle English GPT-2 to Make Models for Other Langs
Wietse de Vries, Malvina Nissim
Findings of the ACL-IJCNLP 2021 arXiv:2012.05628 [cs.CL]

* Large generative language models have been very successful for English, but
* other languages lag behind, in part due to data and computational limitations
* We propose a method: adapting existing pre-trained models to new languages
  * Specifically, the adaptation of English GPT-2 to Italian and Dutch by
  * retraining lexical embeddings without tuning the Transformer layers
  * Additionally, we scale up complexity by transforming relearned lexical
    embeddings of GPT-2 small to the GPT-2 medium embedding space. This method
    minimises the amount of training and prevents losing information during
    adaptation that was learned by GPT-2. English GPT-2 models with relearned
  * can generate realistic sentences in Italian and Dutch
    * identifiable as artificial by humans, they are
    * assessed on par with sentences generated by a GPT-2 model
      fully trained from scratch
