Is BERT Really Robust?
  A Strong Baseline for Natural Language Attack on Text Classif and Entailment
Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits
AAAI 2020 (Oral) Vol. 34 No. 05: AAAI-20 Technical Tracks NLP arXiv:1907.11932 

code pre-trained target models, and test examples are available 

# Abstract

* adversarial examples that have imperceptible alterations from the original
  counterparts but can fool the SOTA models. It is helpful to
* we present TextFooler, a simple but strong baseline to generate natural
  adversarial text. By applying it to two fundamental natural language tasks,
* exper in  text classification and textual entailment, 
  * we successfully attacked three target models, including the powerful
    * BERT, and the widely used convolutional and recurrent neural networks.
* advantages of this framework in three ways: 
  * effective---it outperforms SOTA attacks 
    in terms of success rate and perturbation rate, 
  * preserves semantic content and grammaticality
    * remains correctly classified by humans, and 
  * computational complexity linear to the text length
