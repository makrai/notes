The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey
Y Huang, X Feng, X Feng, B Qin
arXiv preprint arXiv:2104.14839, 2021 arxiv.org

# Abstract

* neural encoder-decoder models pioneered by Seq2Seq framework generating more
  abstractive summaries by learning to map input text to output text. At a high
  * their format is closer to human-edited summaries and output is more readable
* hE, distortion or fabrication of factual information in the article. This in-
  * previous evaluation methods of text summarization are not suitable for this
* In response, the current research direction is predominantly divided into two
  * fact-aware evaluation metrics to select outputs without factual
  * new summarization systems towards factual consistency
* we presenting a comprehensive review of these

# 1 Introcduction

* factual inconsistency errors: the summary sometimes distorts or fabricates fct
  * up to 30% of the summaries generated by abstractive models contain such
    factual inconsistencies [Kryscinski+ 2020; Falke+ 2019].  This brings
* most existing summarization evaluation tools calculate N-gram overlaps
  * Zhu+ [2020] point that the generated summaries are often high in token level
    metrics like ROUGE [Lin, 2004] but lack factual correctness. For instance,
* In the past three years, more than twenty studies on factual consistency of
  summarization have been proposed.  Considering the large amount of effort

# 2 Background

## 2.1 abstractive summarization. Then we focus on the

* Conventional abstractive summarization methods usually
  extract some keywords from the source document, and then
  reorder and perform linguistically motivated transformations to these keywords
  [Zajic+ 2004].  However, previous paraphrase-based generation methods are
  * easy to produce influent sentences [Hahn and Mani, 2000]
* encoder and decoder
  * Nallapati+ [2016] first proposed to use an RNN
    * RNN encoder to encode the source document into a sequence of word vectors,
    * another RNN (i.e.  decoder) to generate a sequence of words as the summary
  * CNN [Narayan+ 2018] and Transformer [Vaswani+ 2017]. The
    * decoder is a conditional language model: readable and fluent [Fan+ 2018]
    * mostly trained to maximize the log-likelihood of the reference summary at
      the word-level, which does
      not necessarily reward models for being faithful [Maynez+ 2020]

## 2.2 factual inconsistency problem by giving

### Intrinsic hallucination Maynez+ [2020]: a fact that contradicts the source

* e.g. the word “central”, which is contradicted to “north” in the source

### Extrinsic hallucination: a fact that is neutral to the source document

* neither supported nor contradicted by the source document), which is also
* e.g. “killing at least seven people and injuring more than 100”, which is not
* existing factual consistency
  * optimization methods mainly focus on intrinsic errors, and
  * evaluation metrics: these two categories are not distinguished

# 3 factual consistency evaluation metrics and meta-evaluation for these metrics

* We divide factual consistency metrics into unsupervised and weakly supervised,
  * Unsupervised metrics use existing tools to evaluate factual consistency of
    * According to tools that unsupervised metrics base on, we further divide to 4
      * Triple-based, Textual entailment-based, QA-based and Others
  * Weakly supervised metrics need to train on the factual consistency data,
    * documents, model-generated summaries, and factual consistency scores for
* Metaevaluations To compare factual consistency metrics with each other, for
  factual consistency rise up. We introduce 2 meta-evaluation works about
  factual consistency. Besides, we organize
* existing metrics into Table 1. There is code for
  QAGS [Wang+ 2020] FEQA [Durmus+ 2020] FactCC [Kryscinski+ 2020]

## Unsupervised Metrics

### Triple-based

* Facts are represented by relation triples (subject, relation, object), where
* To extract triples, Goodrich+ [2019]
  * first try to use OpenIE [Banko+ 2007]
    * unspecified schema i.e. no pre-defined relations set, which could be viewed
  * use relation extraction tools with fixed schema. Considering still the two

### Textual entailment-based

* Falke+ [2019] propose to use textual entailment prediction tools to evaluate
  * also known as Natural Language Inference (NLI), aims at detecting whether a
  * out-ofthe-box entailment models do not yet offer the desired performance for
    * reasons
      * domain shift from the NLI dataset to the summarization dataset. Another
      * NLI models tend to rely on heuristics such as lexical overlap to explain
* Mishra+ [2020]
  * conjecture that a key difference between the NLI datasets and downstream
    tasks concerns the length of the premise. Specifically, most
    * NLI: at most a few sentences as the premise. However, most downstream NLP
    * text summarization and question answering consider a longer premise, which
      * Reasoning over longer text needs a multitude of additional abilities
        like coreference resolution and abductive reasoning. To bridge this gap,
  * create new long premise NLI datasets out of existing QA datasets for

### QA-based

* Inspired by other question answering (QA) based automatic metrics in text summ
* Wang+ 2020 and Durmus+ [2020] propose metrics QAGS and FEQA separately. These
  * based on the intuition that if we ask questions about a summary and its
    source document, we will receive similar answers if the summary is factually
    consistent with the sourcedocument. As illustrated in Figure 4, they are all
  * three steps:
    * Given a generated summary, a question generation (QG) model generates a
      set of questions about the summary, standard answers of which are named
      entities and key phrases in the summary
    * Then using question answering (QA) model to answers these questions given
      the source document
    * A factual consistency score is computed based on the similarity of
      corresponding answers
  * Because evaluating factual consistency at entity-level, these methods are
    more interpretable than textual-entailment-based methods. The
    * reading comprehension ability of QG and QA models brings these methods
      promising performance in this task.  However, these approaches are
  * computationally expensive

### Others [Baselines]

* simple but effective methods to evaluate factual consistency, which are
  usually used as baselines
* Durmus+ [2020]: word overlap or semantic similarity between the summary
  * word overlap-based metrics
    * compute ROUGE [Lin, 2004], BLEU [Papineni+ 2002],
      between the output summary sentence and each source sentence. And then
      taking the average score or maximum score across all the source sentences
  * semantic similarity-based metric is similar to word overlap-based methods
    * Instead of using ROUGE or BLEU, this method uses BERTScore [Zhang+ 2020a]
  * word overlap-based methods work better in lowly abstractive summarization
    datasets like CNN/DM [Hermann+ 2015],
    semantic similarity-based method works better in highly abstractive
    summarization datasets like XSum [Narayan+ 2018]. Abstractiveness of the

## 3.2 Weakly Supervised Metrics

* models specially for evaluating factual consistency. And these models are
* trained on synthetic data that are generated from the summarization dataset
* effect is very limited

### Sentence-level

* Kryscinski+ [2020] propose FactCC, a model to verify the factual consistency
  * fine-tuning BERT [Devlin+ 2019] as a binary classifier. And they further
  * generate synthetic training data from the summarization dataset CNN/DM
    * sampling single sentences, later referred to as claims, from the source
    * positive examples are obtained through semantically invariant
      transformations like paraphrasing.  The
    * negative examples are obtained through semantically variant
      transformations like sentence negation and entity swap. In the test stage,
* it is hard to simulate all types of factual inconsistency errors

### Entity-level

* Zhao+ [2020] propose HERMAN, which focuses on evaluating factual consistency
  of quantity entities (e.g. numbers, dates, etc). HERMAN bases on
* sequence labeling architecture, in which input is the source doc and summary,
  the output is a sequence of labels indicating which tokens consist of factual
  inconsistent quantity entities. The synthetic
* training data for HERMAN is automatically generated from the summarization
  dataset XSum [Narayan+ 2018].  Rather than sampling document sentences as
  * reference summary as claims. And these claims are directly labeled as positv
  * negative summaries are obtained by replacing the quantity entities in
    positive summaries

### Token-level

* Zhou+ [2020] propose to evaluate factual consistency on token-level, which is
* more fine-grained and more explainable than sentence-level and entity-level
* implemented by fine-tuning pre-trained language model. Like Zhao+ [2020],
* reference summaries are also directly labeled as positive examples, and the
  negative examples are obtained by reconstructing part of reference summaries
* higher correlations with human factual consistency evaluation

## 3.3 Meta Evaluation

* most related works usually report the correlation
  between their own metric and human-annotated factual consistency score
* hE, it is still hard to compare each metric by the correlations as the
  * diversity of annotating settings in different works and the
  * disagreement among different annotator groups. To measure the effectiveness
* Gabriel+ 2020 and Koto+ [2019] conduct meta-evaluations of factual consistency
  * correlation between with scores measured by the same group of annotators
* Koto+ [2020] find that the semantic similarity-based method could reach SOTA
  performance for factual consistency evaluation
  by searching optimal model parameters (i.e. model layers of pre-trained
  language model in BERTScore) in highly abstractive summarization dataset XSum
  [Narayan+ 2018].  Even so, the
* correlation with human evaluation is not more than 0.5.  Therefore, factual

# 4 optimizing factual consistency for summarization systems

* we organize these methods into Table 2.  There is code for
  ASGARD [Huang+ 2020], Li+ [2018], Zhang+ [2020b], and Yuan+ [2020]

## fact encode-based

* In the earliest research about factual consistency, most works mainly focus on
  intrinsic factual inconsistency errors, i.e., the fact that is inconsistent
  with the source document. Intrinsic factual inconsistency errors convey
  wrongly the fact of the source document, which
* usually as cross-combinations of the semantic units in different facts. For
  e.g. “Jenny likes dancing. Bob likes playing football.”
  ⇒ “Jenny likes playing football”.  It is the root cause for intrinsic errors
* explicitly model the facts in the source document, to augment the
  representation of facts. Following this idea, fact encode-based methods first
  extract facts in the source document, which are usually represented by
  relation triples consisting of (subject; relation; object). Then, these
* encode the extracted facts into summarization models
* According to the ways to encoding facts, these methods are divided into two:

### sequential fact encode

* Cao+ [2018] introduce FTSum, which consists of
  two RNN encoders and one RNN decoder. FTSum
* concatenates the facts in the source document into a fact description
* One encoder encodes the source document and another encoder encodes the fact
  description. The decoder attends the outputs from these two encoders when
  generating the summary. Even though experimental results show that FTSum
  reduces significantly factual inconsistent errors,

### graph-based fact encode

* to capture the interactions between entities in all facts
* Zhu+ 2020 and Huang+ [2020] propose to model the facts with knowledge graphs
* FASum (Fact-Aware Summarization) [Zhu+ 2020], a transformer-based summ model,
  * a graph neural network (GNN) to learn the representation of each node (i.e.,
    entities and relations) and fuses them into the summarization model
* ASGARD (Abstractive Summarization with Graph Augmentation and semantic-driven
  RewarD) Huang+  [2020] further uses multiple choice cloze reward to
  drive the model to acquire semantic understanding over the input
* incorporating commonsense knowledge is also useful to facilitate summarization
  * Gunel+ [2019] sample relation triples from Wikidata to construct a
    commonsense knowledge graph. In this knowledge graph,
  * TransE [Bordes+ 2013], the popular multi-relational data modeling method, is
    used to learn entity embeddings. And
  * the summarization system attends to the embedding of related entities when
    encoding the source document. In this way, commonsense knowledge is incorpor

## textual entailment-based

* [Li+ 2018] aim at incorporating entailment knowledge into the summ model
  * a pair of entailment-aware encoder and decoder. The entailmentaware
  * encoder is used to learn simultaneously summary generation and textual
    entailment prediction. And the entailment-aware
  * decoder is implemented by entailment Reward Augmented Maximum Likelihood
    (RAML) training.  RAML [Norouzi+ 2016] provides a computationally efficient
    approach to optimizing task-specific reward (loss) directly. In this model,
    the reward is the entailment score of generated summary

## post-editing-based

* Fact correctors take the source document and draft summary as input and
  generates the corrected summary as the final summary
* Inspired by the QA span selection task, Dong+ [2020] propose SpanFact, a suite
  of two span selection-based fact correctors, which
  corrects the entities in the draft summary
  in an iterative resp. auto-regressive manner
  * Before performing fact correcting, one resp. all entities will be masked
  * Then SpanFact selects spans in the source document to replace corresponding
    mask tokens based on the understanding of the source document
  * successfully corrects about 26% factually inconsistent summaries and
    wrongly corrupts less than 1% factually consistent summaries. However,
* Simpler than SpanFact, Cao+ [2020] propose an Endto-End fact corrector, which
  * can correct more types of errors
  * by fine-tuning pre-trained language model BART [Lewis+ 2020] with artificial
    data. It takes a
    * corrupted summary as input. The output is the corrected summary. Even
      though this method could correct more factually inconsistent errors than
      Span-Fact conceptually, it has
  * not outperform SpanFact in human evaluation result
* Both of Dong+ [2020] and Cao+ [2020] choose to construct artificial training
  data automatically instead of using expensive human annotation
* hE, gap between the
  * training stage (which learns to correct the corrupted reference summaries) and
  * testing stage (which aims to correct the model-generated summaries)
  limits the performance of post-editing-based methods for the reason that
  corrupted reference summaries have a different data distribution with the
  model-generated summaries, which is
  * the same as weakly supervised factual consistency metrics (§3.2)

## other methods. simple but useful methods and domain-specific methods

* Matsumaru+ [2020] conjecture that one of the causes of factually inconsistent
  summaries lies in unfaithful document-summary pairs, which are used for
  training the model. To mitigate this issue, they further propose to filter
  inconsistent training examples with a textual entailment classifier
* Mao+ [2020] apply constraints during the inference (i.e., beam search) stage
  * end decoding only when all the constraints are met.  And the
  * constraints are important entities and keyphrases. Because this method only
  * hE, how much improvement of factual consistency could be achieved by this
    method and how to design more useful constraints are still questions
* As opposed to relatively open domain summarization, such as news field,
  optimization approaches towards factual consistency in special field are more
  different for their field characters. In the
  * medical field, Zhang+ [2020b] propose to optimize the factual consistency of
    radiology reports summarization
  * Shah+ [2021] propose to optimize the factual consistency of health and nutri
  * Yuan+ [2020]: e-commerce product
