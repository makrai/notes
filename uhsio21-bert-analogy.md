BERT is to NLP what AlexNet is to CV: Can Pre-Trained LMs Identify Analogies?
Asahi Ushio, Luis Espinosa Anke, Steven Schockaert, Jose Camacho-Collados
ACL | IJCNLP 2021

# Abstract

* Analogies play a central role in human commonsense reasoning. The ability to
  * analogical proportions eg “eye is to seeing what ear is to hearing”,
  * shape how we structure knowledge and understand language. Surprisingly,
* we analyze the capabilities of transformer-based language models on this
  unsupervised task, using
  * benchmarks from educational settings, as well as more commonly used ones
* off-the-shelf language models can identify analogies to a certain extent, but
  * struggle with abstract and complex relations, and
  * results are highly sensitive to model architecture and hyperparameters.
  * best results were obtained with GPT-2 and RoBERTa, while configurations
    * BERT were not able to outperform word embedding models. Our results raise
