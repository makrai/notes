Neural text summarization for Hungarian
ZIJIAN GYŐZŐ YANG1,2p
Acta Linguistica Academica 69 (2022) 4, 474–500 DOI: 10.1556/2062.2022.00577

# 7 ABSTRACTIVE SUMMARIZATION EXPERIMENTS AND RESULTS

* The most notable result is that the fine-tuned mBART-50 model could gain the
  highest results in several tasks despite the lack of Hungarian pre-training.
* Another intriguing result to be emphasized is that the further fine-tuned
  huBERT-based foszt2foszt, which has fewer parameters, could achieve the
  highest performance (ROUGE-2, ROUGE-L) on HI corpus. It means the fewer
  parameters are outweighed by the “well-taught” Hungarian pre-trained
  knowledge
* On NOL corpus, the ﬁne-tuned mT5 model could gain the highest performance.
* message: different model types could achieve different performance on
  different data types. It is not possible to clearly determine which model
