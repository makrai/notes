Attention Is All You Need
A Vaswani, L Jones, N Shazeer, N Parmar, J Uszkoreit, Ł Kaiser, I Polosukhin
NIPS 2017

# Abstract

* the Transformer
* Experiments on two machine translation tasks show these models to be 
  superior in quality while being 
  more parallelizable and 
  requiring significantly less time to train
* generalizes well ... to English constituency parsing 
  both with large and limited training data

# Attention Visualizations 13

* long-distance dependencies in the encoder self-attention in layer 5 of 6.
  ... completing the phrase ‘making...more difficult’
* anaphora resolution
