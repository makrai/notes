Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, Tie-Yan Liu
Probabilistic Model for Learning Multi-Prototype Word Embeddings
2013

* Some recent studies attempted to
  * train multi-prototype word embeddings
    through clustering context window features
  * limited scalability and are inefficient to be trained with big data
* this paper, we
  * introduce a much more efficient method for learning multi embedding vectors
  * propose to model word polysemy from a probabilistic perspective and
  * integrate it with the highly efficient continuous Skip-Gram model
  * an Expectation-Maximization algorithm
  * results on word-similarity tasks

* quantitative evaluation results on a public word similarity task
