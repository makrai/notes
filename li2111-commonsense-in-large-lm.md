A Systematic Investigation of Commonsense Understanding in Large Language Models
Xiang Lorraine Li, Adhi Kuncoro, Cyprien de Masson d'Autume, Phil Blunsom, Aida Nematzadeh
arXiv:2111.00607 [cs.CL]

Large language models have shown impressive performance on many natural
language processing (NLP) tasks in a zero-shot setting. We ask whether these
models exhibit commonsense understanding -- a critical component of NLP
applications -- by evaluating models against four commonsense benchmarks. We
find that the impressive zero-shot performance of large language models is
mostly due to existence of dataset bias in our benchmarks. We also show that
the zero-shot performance is sensitive to the choice of hyper-parameters and
similarity of the benchmark to the pre-training datasets. Moreover, we did not
observe substantial improvements when evaluating models in a few-shot setting.
Finally, in contrast to previous work, we find that leveraging explicit
commonsense knowledge does not yield substantial improvement. 
