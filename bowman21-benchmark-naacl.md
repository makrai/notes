What Will it Take to Fix Benchmarking in Natural Language Understanding?  
Samuel R. Bowman, George Dahl
NAACL 2021

# Abstract

* Evaluation for many natural language understanding (NLU) tasks is broken:
* Unreliable and biased systems score so highly on standard benchmarks that
  there is little room for researchers who develop better systems to demonstrate
* The recent trend to abandon IID benchmarks in favor of adversarial
  out-of-distribution test sets ensures that current models will perform poorly,
  but ultimately only obscures the abilities that we want to measure
* In this position paper, we lay out four criteria that we argue NLU benchmarks
* most current benchmarks fail at these criteria, and that 
  * adversarial data collection does not meaningfully address the causes of
    these failures
* restoring a healthy evaluation ecosystem will require significant progress in
  the design of benchmark datasets, the reliability with which they are annoted,
  their size, and the ways they handle social bias
