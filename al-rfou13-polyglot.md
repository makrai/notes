Rami Al-Rfou’ Bryan Perozzi Steven Skiena
2013
Polyglot: Distributed Word Representations for Multilingual NLP

TODO
    a cikk óta volt-e előrelépés?

# 2 Related Work

* variety of unsupervised feature learning methods ...  unilaterally improve
  the performance of supervised learning tasks (Turian+ 2010)
* to speed up the training 2012-
* SENNA: classification
* Chen+ (2013) show that the embeddings generated by SENNA perform well in a
  variety of term-based evaluation tasks
* we: similar network architecture to the one SENNA used

# 3 Distributed Word Representation

* predict the last word of a phrase that consists of n words
* In our work, we start from the example construction method outlined in
  (Bengio+ 2009)
* corrupted sequence S by replacing the word in the middle
* tanh(W 1 P + b 1 )
* score(P ) = W 2 A + b 2
* 2n + 1
* output: 1 score

# 4 Corpus Preparation

* process Wikipedia markup, we first extract the text using a modified version
  of the Bliki engine
* OpenNLP probabilistic tokenizer whenever possible, and
  default to the Unicode text segmentation2 algorithm offered by Lucene when no
* normalization
  * 1999 becomes ####
  * remove hyphens and brackets that appear in the middle of a token
  * for English, we map non-Latin characters to their unicode block
  * groups
  * morphology

# 5 Training

* parameters
  * window size 2n + 1 = 5,
  * vocabulary |V | = 100, 000,
  * word embedding size M = 64, and
  * hidden layer size H = 32
* corpus
  * unknown words are replaced with a special token UNK and sentences are
    padded with S , /S tokens. In case the window exceeds the edges of a
    sentence, the missing slots are filled with our padding token, PAD
* learning
  * minibatches of size 16. Every 16 examples, we estimate the gradient
  * learning rate to be 0.1
  * Following, (Collobert+ 2011)’s advice, we divide each layer
  * by the fan in of that layer, and..
  * stopping criteria
* we build a model as the one described in Section 3
  * using Theano (Bergstra+ 2010)
* Another benefit is that we will avoid overfitting on the smaller
* Wikipedias
* érdeklődés: Rumelhart+ 2002 (recent about back-propagation)
* Following, (Collobert+ 2011)’s advice,
  * we divide each layer by the fan in of that layer, and we consider the
    embeddings layer to have a fan in of 1

# 6 Qualitative Analysis

* Euclidean

# 7 Sequence Tagging

* Here we analyze the quality of the models we have generated
* part of speech tagging
  * architecture
    * hidden layer of size 300 and
    * learning rate of 0.3
    * minimizing the negative of the log likelihood of the labeled data
    * `2n = 2*2`
* universal tagset proposed by (Petrov+ 2012)
* (we do not add OOV words seen during training)

# 8 Conclusion

* We hope that these resources will advance the study of possible pair-wise
  mappings between embeddings of several languages and their relations
