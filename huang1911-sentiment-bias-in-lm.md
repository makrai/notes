Reducing Sentiment Bias in Language Models via Counterfactual Evaluation
Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, Pushmeet Kohli
Findings of EMNLP, 2020 arXiv:1911.03064 [cs.CL]

# Abstract

* text generation models internalize social biases present in the training
* we quantify and reduce bias in the sentiment of generated text.  
* Given a conditioning context (e.g., a writing prompt) and a language model,
  * changes in values of sensitive attributes (e.g., country names,
    occupations, genders) in the conditioning context 
  * counterfactual evaluation. We quantify sentiment bias by adopting 
* individual and group fairness metrics from the fair machine learning literat,
* large-scale models trained on two different corpora (news articles, and
  Wikipedia) exhibit considerable levels of bias.
* We then propose embedding and sentiment prediction-derived regularization on
  the language model's latent representations
