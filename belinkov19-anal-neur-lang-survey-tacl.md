Yonatan Belinkov and James Glass
Analysis Methods in Neural Language Processing: A Survey
TACL Volume 7, 2019 p.49-72

* detailed references for studies corresponding to Sections 2, 4, and 5
  (Tables SM1, SM2, and SM3, respectively), available at
  http://boknilev.github.io/nlp-analysis-methods

#Abstract

#Intro

* [deep learning in NLP ...  improved] various tasks, including
  * language modeling (Mikolov+ 2010; Jozefowicz+ 2016),
  * syntactic parsing (Kiperwasser and Goldberg, 2016),
  * machine translation (MT) (Bahdanau+ 2014; Sutskever+ 2014), and
  * many other tasks; see Goldberg (2017) for example success stories
* two trends
  * incorporating [linguistic knowledge ] inside the networks
  * understand how NLP models work
* interpretability in machine learning, along with [NLP-spec characteristics]
  * accountability, trust, fairness, safety, and reliability
    (Doshi-Velez and Kim, 2017; Lipton, 2016)
* earlier NLP work, often referred to as feature-rich or feature-engineered
  * morphological properties, lexical classes, syntactic categories, sem rels
  * In theory, one could observe the importance assigned by statistical NLP
    models to such features in order to gain a better understanding

#2 [analysis] work

* We also point to limitations in current methods for answering this question

#3 visualization methods

* difficulty in evaluating visualization work

#4 compilation of challenge sets, or test suites, for fine-grained evaluation

#5 adversarial examples

* dealing with text as a discrete input and how different studies handle them

#6 explaining model predictions

* relatively underexplored area, and we call for more work in this direction

#7 a few other methods

#8 conclusion ... and potential research directions
