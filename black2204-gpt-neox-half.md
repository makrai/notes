GPT-NeoX-20B: An Open-Source Autoregressive Language Model
S Black, S Biderman, E Hallahan, Q Anthony, L Gao, L Golding, H He, C Leahy,
  K McDonell, J Phang, M Pieler, U Sai Prashanth, S Purohit, L Reynolds, et al
ACL Workshop on ... Creating Large Language Models arXiv:2204.06745

# Abstract

* We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive LM
  * trained on the Pile, whose
  * freely and openly available to the public through a permissive license. It
  * the largest dense autoregressive model that has publicly available weights
* we describe \model{}'s architecture and training and
  * evaluate its performance on a range of language-understanding, mathematics,
    and knowledge-based tasks.  We find that GPT-NeoX-20B is a
  * particularly powerful few-shot reasoner and gains
  * far better five-shot perf than similarly sized GPT-3 and FairSeq models. We
