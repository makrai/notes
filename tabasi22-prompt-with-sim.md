Exploiting Language Model Prompts Using Similarity Measures:
  A Case Study on the Word-in-Context Task 
Mohsen Tabasi, Kiamehr Rezaee, Mohammad Taher Pilehvar
ACL 2022

# Abstract

* As a recent development in few-shot learning, prompt-based techniques have
  * promising potential in a variety of natural language processing tasks.
  * competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing
  * hE fail on the semantic distinction task of the Word-in-Context (WiC) data
    * not signif better than random
    * noof the few-shot approaches (including the in-context learning of GPT-3)
* we propose a new prompting technique, based on similarity metrics, which
  * boosts few-shot performance to the level of fully supervised methods. Our
  * shows that the failure of prompt-based techniques in semantic distinction
    is due to their improper configuration,
    rather than lack of relevant knowledge in the representations.  We also
  * approach can be effectively extended to other downstream tasks
    for which a single prompt is sufficient.
