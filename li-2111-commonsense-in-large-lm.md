A Systematic Investigation of Commonsense Understanding in Large LMs
Xiang Lorraine Li, Adhi Kuncoro, C de M d'Autume, Phil Blunsom, Aida Nematzadeh
arXiv:2111.00607 [cs.CL]

# Abstract

* impressive zero-shot performance of Large language models on many NLP tasks
* We ask whether these models exhibit commonsense understanding -- a critical
* evaluating models against four commonsense benchmarks. We find that the
* performance is mostly due to dataset bias in our benchmarks. We also show
* performance is sensitive to the
  choice of hyper-parameters and
  similarity of the benchmark to the pre-training datasets. Moreover, we did
* no substantial improvements when evaluating models in a few-shot setting
* in contrast to previous work, we find that leveraging explicit commonsense
  knowledge does not yield substantial improvement
