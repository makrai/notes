SummEval: Re-evaluating Summarization Evaluation
AR Fabbri, W Kryściński, B McCann, C Xiong, R Socher, D Radev
arXiv:2007.12626

https://github.com/Yale-LILY/SummEval

* scarcity of comprehensive up-to-date studies on evaluation metrics for summ
* lack of consensus regarding evaluation protocols
* We address these shortcomings along 5 dims:
  * we re-evaluate 12 automatic evaluation metrics
    * using neural summarization model outputs along with
      expert and crowd-sourced human annotations,
  * we consistently benchmark 23 recent summarization models
  * we share
    * the largest collection of summaries generated by models trained on the
      CNN/DailyMail news dataset and share it in a unified format,
    * toolkit that provides an extensible and unified API for evaluating
      a broad range of automatic metrics,
    * collection of human judgments of model-generated summaries on the CNN/Dail
      * the largest and most diverse in terms of model types
      * annotated by both expert judges and crowd source workers

# 1 Introduction

* Text summarization has benefited from advances in
  * neural network architectures
    (Sutskever+ 2014; Bahdanau+ 2014; Vinyals+ 2015; Vaswani+ 2017) as well as
  * large-scale datasets
    (Sandhaus, 2008; Hermann+ 2015; Grusky+ 2018; Narayan+ 2018)
  * pretrained language models, such as BERT (Liu and Lapata, 2019;
    Zhang+ 2019b; Dong+ 2019; Ziegler+ 2019; Raffel+ 2019; Lewis+ 2019)
* CNN/DailyMail corpus (Hermann+ 2015),
  * A standard dataset for training summarization models
  * originally a question answering task, which was
  * repurposed for summarization by Nallapati+ (2016)
  * news articles and associated human-created bullet-point summaries
* ROUGE (Lin, 2004b) metric, which measures lexical overlap
  * typically used together with crowd-sourced human annotations for eval
* the current setup has become standardized, hE
* papers vastly differ in their evaluation protocol (Hardy+ 2019)
  * often limited to a few baselines
  * human evaluations which are largely inconsistent with prior work
* ROUGE
  * problems when used outside of its original setting
    (Liu and Liu, 2008; Cohan and Goharian, 2016)
  * many variants (Zhou+ 06; Ng and Abrecht 15; Ganesan 15; ShafieiBavani+ 18)
  * other text generation metrics
    (Peyrard, 2019; Zhao+ 2019; Zhang+ 2020; Scialom+ 2019; Clark+ 2019)
  * remained the default automatic evaluation metric
* lack of easy-to-use resources for evaluation:
  no simplified evaluation toolkits and no large collections of model outputs
* how evaluation metrics are evaluated themselves
  * Many of the currently used metrics were developed and assessed using the
    Document Understanding Conference (DUC) and Text Analysis Conference (TAC)
    shared-tasks datasets (Dang and Owczarzak, 2008, 2009)
  * hE, these datasets contain human judgments scoring on a lower scale
    compared to current summarization systems (Peyrard, 2019)
    putting into question the true performance of those metrics in the new settn
* We address these gaps in complementary ways:
  * We re-evaluate 12 automatic evaluation metrics
    * outputs from recent neural summarization models along with
      expert and crowd-sourced human annotations,
  * We consistently benchmark 23 recent summarization models
  * We release 44 aligned summarization model outputs from 23 papers
    published between 2017 and 2019 trained on the CNN/DailyMail dataset
    * for large-scale comparisons of recent summarization models,
  * We release a toolkit of 12 evaluation metrics with an
    extensible and unified API to promote the reporting of additional metrics
  * We collect and release expert, as well crowd-sourced, human judgments
    for 16 model outputs on 100 articles over 4 dimensions to further research

# 2 Related Work

* Previous work examining the research setup of text summarization
  * three groups, based on the subject of analysis:
    evaluation metrics, datasets, and models

## metrics

* Lin (2004a) examined the effectiveness of ROUGE in various DUC tasks
  * evaluating against multiple references results in higher corr with human, hE
  * single-reference setting is sufficient for the metric to be effective
* Owczarzak+ (2012) studied the effects of inconsistencies in human annotations
  on the rankings of evaluated summarization systems
  * system-level rankings were robust against annotations inconsistencies, hE
  * summary-level rankings were not stable in such settings
* Rankel+ (2013) analyzed variants of the ROUGE metric using TAC datasets
  * higher-order and less commonly reported ROUGE settings showed
    higher correlation with human judgments
* similarly, Graham (2015)
  * large-scale study of the effectiveness of different ROUGE metric variants
  * compared against the BLEU metric on the DUC datasets
  * Its results highlighted several superior, non-standard ROUGE settings that
    achieved strong correlations with human judgements on model-generated summs
* Chaganty+ (2018) investigated using an automatic metric to
  reduce the cost of human evaluation without introducing bias
  * released a set of human judgments over several model outputs,
    limited to a small set of model types
* Peyrard (2019) showed that
  * standard metrics are in agreement
    when dealing with summaries in the scoring range found in TAC summaries, but
  * vastly differ in the higher scoring range found in current models
  * additional human annotations on modern model outputs are necessary to
    conduct a conclusive study of evaluation metrics
* Hardy+ (2019)
  * underscore the differences in approaches to human summary evaluation while
  * proposing a highlight-based reference-less evaluation metric
* problems with applying ROUGE in settings such as
  * meeting summarization (Liu and Liu, 2008) and summarization of
  * scientific articles (Cohan and Goharian, 2016)

## datasets

* Dernoncourt+ (2018) presented a detailed taxonomy of summ datasets
  * differences in formats of available corpora and
  * call for creating a unified data standard
* similarly, Grusky+ (2018) offered a thorough analysis of corpora,
  * focusing on news summarization datasets
  * introduced several metrics for evaluating the extractiveness of summaries
    * included in the toolkit implemented as part of this work
* Kryściński+ (2019a)
  * news-related summarization datasets, such as CNN/DailyMail, contain
    strong layout biases
  * task of summarization underconstrained
    * where each news article is associated with a single reference summary,
  * noisy, low quality data in automatically collected news datasets

## models

* Zhang+ (2018a): level of abstraction of several recent abstractive summ models
  * word-level extractive models achieved a similar level of abstraction to
    fully abstractive models
* Kedzie+ (2018) examined the influence of various model components in extractiv
  * in the current setting the training signal is dominated by biases present in
    summarization datasets preventing models from learning accurate selection
* factual correctness, faithfulness
  * Kryściński+ (2019b)
    * hallucinating facts touches up to 30% of generated summaries
    * common types of errors made by generative models
  * Maynez+ (2020) conducted a
    * large-scale study of abstractive summarizers' faithfulness
    * improving factual faithfulness is a critical issue in summarization
    * ROUGE and BertScore are not sufficient to study the problem at hand
  * Durmus+ (2020) and Wang+ (2020) similarly examine faithfulness evaluation,
    * proposing question answering frameworks as a means of evaluation

* library for developing summarization metrics (Deutsch and Roth 2020)
  * only 3 of our 12 evaluation metrics

# 3 Evaluation Metrics and Summarization Models 4

## 3.1 Evaluation Metrics

* Our selection includes
  * several recent metrics applied to both text generation and summarization,
  * standard machine translation metrics, and
  * other miscellaneous performance statistics
* ROUGE (Lin, 2004b, Recall-Oriented Understudy for Gisting Evaluation),
  measures the number of overlapping textual units (n-grams, word sequences)
  between the generated summary and a set of gold reference summaries
* ROUGE-WE (Ng and Abrecht, 2015) extends ROUGE by using soft lexical matching
  based on the cosine similarity of Word2Vec (Mikolov+ 2013)
* S^3 (Peyrard+ 2017) is a model-based metric that uses previously proposed
  evaluation metrics, such as ROUGE, JS-divergence, and ROUGE-WE, as input
  features for predicting the evaluation score
  * trained on human judgment datasets from TAC conferences
* Bert-Score (Zhang+ 2020) computes similarity scores
  * by aligning generated and reference summaries on a token-level
  * Token alignments are computed greedily with the objective of maximizing the
    cosine similarity between contextualized token embeddings from BERT
* MoverScore (Zhao+ 2019) measures semantic distance between a summary and
  reference text by making use of the Word Mover’s Distance (Kusner+ 2015)
  operating over n-gram embeddings pooled from BERT representations
* Sentence Mover’s Similarity (SMS, Clark+ 2019) extends Word Mover’s Distance
  to view documents as a bag of sentence embeddings as well as
  a variation which represents docs as both a bag of sents and a bag of words
* SummaQA (Scialom+ 2019) applies a BERT-based question-answering model to
  answer cloze-style questions using generated summaries
  * Questions are generated by masking named entities in source documents
    associated with evaluated summaries
  * reports both the F1 overlap score and QA-model confidence
* BLEU (Papineni+ 2002) is a corpus-level precision-focused metric which
  calculates n-gram overlap between a candidate and reference utterance and
  includes a brevity penalty
  * the primary evaluation metric for machine translation
* CHRF (Popović, 2015) calculates character-based n-gram overlap
  between model outputs and reference documents
* METEOR (Lavie and Agarwal, 2007) computes an alignment between candidate and
  reference sentences by mapping unigrams in the generated summary to 0 or 1
  unigrams in the reference based on stemming, synonyms and paraphrastic match
  * Precision and recall are computed and reported as a harmonic mean
* CIDEr (Vedantam+ 2015) computes {1-4}gram co-occurrences between the candidate
  and reference texts, down-weighting common n-grams and calculating
  cosine similarity between the ngrams of the candidate and reference texts
* Data Statistics: Grusky+ (2018) define three measures of the extractiveness
  * Extractive fragment coverage is
    * the percentage of words in the summary that are from the source article,
      measuring the extent to which a summary is a derivative of a text
  * Density is defined as the average length of the extractive fragment to which
    each summary word belongs
  * Compression ratio is defined as the word ratio between the articles and summ
  * we also include: the percentage of
    * n-grams in the summary not found in the input document: novelty score
    * n-grams in the summary which repeat: redundancy

## 3.2 Summarization models

### Extractive Methods

* M1 NEUSUM (Zhou+ 2018) jointly scores and selects sentences by
  first building a hierarchical representation of a document and
  considering the partially outputted summary at each time step
* M2 BanditSum (Dong+ 2018) treats extractive summarization
  as a contextual bandit problem where the document is the context and
  the sequence of sentences to include in the summary is the action
* M3 LATENT (Zhang+ 2018b) propose a latent variable extractive model which
  views relevance labels of sentences in a document as binary latent variables
* M4 REFRESH (Narayan+ 2018) propose using REINFORCE (Williams, 1992),
  approximating the search space during training
  by limiting to combinations of individually high-scoring sentences
* M5 RNES (Wu and Hu, 2018) propose a coherence model to capture cross-sentence
  coherence, combining output from the coherence model and ROUGE scores as a
  reward in a REINFORCE framework
* M6 JECS (Xu and Durrett, 2019) first extracts sentences from a doc and then
  scores possible constituency-based compressed units to produce the final summ
* M7 STRASS (Bouscarrat+ 2019) extracts a summary
  by selecting the sentences with the closest embeddings to the document embed,
  learning a transformation to maximize the similarity between the summary and
  the ground truth reference

### Abstractive Methods

* M8 Pointer Generator (See+ 2017) propose
  * a variation of encoder-decoder models, the Pointer Generator Network, where
  * the decoder can choose to generate a word from the vocabulary or
    copy a word from the input
  * coverage mechanism is also proposed
    to prevent from repeatedly attending to the same part of the source document
* M9 Fast-abs-rl (Chen and Bansal, 2018) propose a model which
  first extracts salient sentences with a Pointer Network and
  rewrites these sentences with a Pointer Generator Network
  * In addition to maximum likelihood training a ROUGE-L reward is used to
    update the extractor via REIN-FORCE (Williams, 1992)
* M10 Bottom-Up (Gehrmann+ 2018) introduce a bottom-up approach whereby
  * a content selection model restricts the copy attention distribution of a
    pretrained Pointer Generator Network during inference
* M11 Improve-abs (Kryściński+ 2018) extend the model of Paulus+ (2017) by
  augmenting the decoder with an external LSTM language model and
  add a novelty RL-based objective during training
* M12 Unified-ext-abs (Hsu+ 2018) propose to
  use the probability output of an extractive model as sentence-level attention
  to modify word-level attention scores of an abstractive model, introducing an
  inconsistency loss to encourage consistency between these two levels of attent
* M13 ROUGESal (Pasunuru and Bansal, 2018) propose a
  keyphrase-based salience reward as well as an entailment-based reward in
  addition to using a ROUGE-based reward in a REINFORCE setting,
  optimizing rewards simultaneously in alternate mini-batches
* M14 Multi-task (Entl + QG, Guo+ 2018) propose
  * a multi-task framework along with a corresponding multi-task architecture
  * question generation and entailment generation as auxiliary tasks
* M15 Closed book decoder (Jiang and Bansal, 2018)
  build upon a Pointer Generator Network by adding an
  additional copy-less and attention-less decoder during training time
  to force the encoder to be more selective in encoding salient content
* M16 SENECA (Sharma+ 2019) propose to use entity-aware content selection module
  and an abstractive generation module to generate the final summary
* M17 T5 (Raffel+ 2019) perform a systematic study of transfer learning techs
  and apply their insights to a set of tasks all framed as text-input text-outpt
* M18 NeuralTD (Böhm+ 2019) learn a reward function from 2,500 human judgments
  which is used in a reinforcement learning setting
* M19 BertSum-abs (Liu and Lapata, 2019) introduce a
  * novel document-level encoder on top of BERT (Devlin+ 2019), over which they
    introduce both an extractive and an abstractive model
* M20 GPT-2 (Ziegler+ 2019) build off of GPT-2 (Radford+ 2019) and
  * fine-tune GPT-2 by using human labels of which of four sampled summaries is
    the best to direct fine-tuning in a reinforcement learning framework
* M21 UniLM (Dong+ 2019): a model pretrained on three language modeling tasks:
  unidirectional, bidirectional, and sequence-to-sequence prediction
  * applicable to NLU tasks and generation tasks such as abstractive summariz
* M22 BART (Lewis+ 2019): denoising autoencoder for pretraining seq to seq tasks
  * applicable to both NLU and generation tasks
* M23 Pegasus (Zhang+ 2019a) introduce a model pretrained with a novel objective
  function designed for summarization by which important sentences are removed
  from an input document and then generated from the remaining sentences

# 4 Resources collected and released together with this work 6

## 4.1 Model Outputs

* from 23 recent papers on neural text summarization described in Section 3.2
  * 44 model outputs, as many papers include variations of the main model
  * All models were trained on the CNN/DailyMail news corpus
    * using the test split of the dataset
  * unified format and IDs of the original CNN/DailyMail examples
    so that generated summaries can be matched with corresponding src articles
    * Pairing model outputs with original articles was done using a heuristic
      approach that relied on aligning reference summaries
  * 38 examples in the CNN/DailyMail test split contained duplicate reference
    summaries preventing those examples to be correctly aligned
    * hE, this problem involves only 0.3% of the data and should not be signf
    * IDs of duplicate examples are provided together with the data

## 4.2 Evaluation Toolkit

* 12 automatic evaluation metrics described in Section 3.1, a Python package
  * a high-level, easy-to-use interface
  * evaluate_example and evaluate_batch functions
  * standard configuration resembling the most popular settings for each metric
    * each metric can be further configured using external `gin` config files
  * command-line tool to eval a summ model with several metrics in parallel

## 4.3 Human Annotations

* summary evaluations of 16 recent neural summarization models solicited from
  crowd-source and expert judges
* 100 articles randomly picked from the CNN/DailyMail test set
  * each summary was scored by 5 crowd-source and 3 expert workers, amounting to
    12800 summary-level annotations
* Model outputs were evaluated along the following four dimensions,
  as in Kryściński+ (2019a):

### Coherence: the collective quality of all sentences

* aligned with the DUC quality question (Dang, 2005) of structure and coherence
  whereby "the summary should be well-structured and well-organized.  The
  summary should not just be a heap of related information, but should build
  from sentence to sentence to a coherent body of information about a topic."

### Consistency [precision]: the factual alignment between the summary and the source

* A factually consistent summary contains only statements that are entailed by
  the source document

### Fluency: the quality of individual sentences

* again from the DUC quality guidelines, sentences in the summary "should have
  no formatting problems, capitalization errors or obviously ungrammatical
  sentences (e.g., fragments, missing components)

### Relevance: selection of important content from the source

* The summary should include only important information from the source doc
* Annotators were instructed to penalize too long or redundant summaries

### Annotation protocol

* The data collection interface provided judges with the
  source article and associated summaries grouped in sets of 5
  * Each group of summaries contained the reference summary associated with the
    source article in order to establish a common point of reference
  * Summary grouping and order within groups was randomized for each annotator
  * Judges were asked to rate the summaries on a Likert scale from 1 to 5
    (higher better) along the four mentioned dimensions
* Crowd-source annotators were hired through the Amazon Mechanical Turk platform
  * hiring criteria: min 10000 approved HITs and an approval rate of 97%
  * Geographic constrains: United States, United Kingdom, and Australia to
    ensure that summaries were evaluated by native English speakers
  * Compensation was calculated to ensure an average wage of 12 USD per hour
* expert
  * non-expert summary judgments could exhibit worse inter-annotator agreement
    (Gillick and Liu 2010)
  * we enlisted three expert annotators who have written papers on summarization
    either for academic conferences (2) or as part of a senior thesis (1)
  * asked to evaluate the same set of summaries under the same instructions
  * two rounds of annotation
  * In the second round, annotators were asked to check all examples for which
    their score of a dimension differed from another annotator by more than 2
    points and the other annotators were within 1 point of each other
    * In cases where a score differed by more than 2 points for which such a
      pattern did not exist, all annotators examined the annotation
    * to correct any obvious mistakes as well as to confirm judgments

# 5 Metric Re-evaluation 8

## 5.1 Human Annotations

* Gillick and Liu, (2010): quality differences between crowd-sourced and expert
* we study this issue using the human annotations collected in this work
* Krippendorff’s alpha coefficient (Krippendorff, 2011)
  * inter-annotator interval kappa to be below an acceptable range
    for the crowd-source workers and first round of expert annotations
  * second round of expert annotations improved the inter-annotator agreement
* similarity of annotations between the two groups of annotators
  * Pearson’s correlation coefficient close to 0, indicating no correlation
* We also manually inspected the human annotations and present examples of
  annotated summaries as well as the differences in human judgments
  * ambiguous pronoun usage and factual inconsistencies
  * token and phrase repetitions
    * expert annotators resulting in a low fluency score, while
    * crowd-source annotators incorrectly classified them as issues with factual
* difficulties of crowd-sourcing high quality annotations and the necessity
  for protocols for improving human evaluation in text summarization

## 5.2 Automatic Metrics

* Many automatic metrics for eval summarization and other text generation models
* lacks a comprehensive study
* In Table 2 we show the correlations between automatic metrics and human
  * computed using expert annotations to avoid the crowd-sourcing problems
  * multi-reference setting, using the original reference in the CNN/DailyMail
    and 10 additional summaries coming from Kryściński+ (2019a)
    * without differentiating between abstractive and extractive models,
      * most metrics did not exhibit large differences
      * correlation tables with a separation by model type in Appendix
  * most metrics have highest correlation within the relevance dimension
    * correlation strength can be classified as either weak or moderate
    * follows intuition: most metrics explicitly/implicitly calculate token ovrl
  * measures of extractiveness
    * e.g. the percentage of novel n-grams in the summary and extractive coverag
  * correlate moderately with consistency
    * abstraction may be at odds with faithfulness
  * The highest correlation for coherence is found in examining repeated n-grams
    * repetition displays a lack of coherence
  * most metric correlations are considerably worse along coherence and fluency,
* Pearson’s correlation coefficients, pairwise, between all metrics
  * strong correlation between all metrics that compute imply/exply lex overlap
  * n-gram novelty and repetitiveness show weak negative correlation with all
    ROUGE-related metrics
  * Length non-weakly correlated with S^3, which might suggest
    * S^3 may be biased towards longer summaries
  * weak correlation of SummaQA with all other evaluated metrics, which
    calls for an additional investigation

# 6 Model Re-evaluation 9

* model scores across human evaluations and automatic metrics
  * models were released between 2017 and 2019
  * abstractive, extractive and hybrid
  * architectures reflect the trends in summarization research
  * in many cases we obtained multiple variants of the same model
    * we focus on the versions with highest ROUGE-L scores
* Table 3 contains the results of human evaluation across the four dimensions
  * Scores for ground truth summaries are included as a point of reference
  * pretrained models consistently performed best on most dimensions
    * models such as Pegasus, BART, and T5
    * scored highest on consistency and fluency while obtaining
      lower scores for relevance and coherence
  * extractive models: lack of coherence and issues with selecting relevant
  * Abstractive model: increasing trend with respect to the date of publication
  * reference summaries did not score well on consistency
    * often contained extraneous information, such as hyperlinks and click-bait
      descriptions of other articles. As this information was not present in the
      source document, the annotators interpreted it as a hallucination
* Table 4 show scores for model outputs across all automatic evaluation metrics
  * results align with insights coming from human evaluation of models
  * highest scores were assigned to large models pretrained on vast data
  * hE S^3, SummaQA, SMS, CHRF, and METEOR tended to favor extractive models

# 7 Conclusions

* SummEval, a set of resources for summarization model and evaluation research
  * collection of summaries generated by recent summarization models on the
    CNN/DailyMail dataset
  * toolkit for summarization model evaluation, and a
  * diverse collection of human annotations of model outputs collected from the
    crowd-source and expert annotators
* we re-evaluated a broad selection of current models and evaluation metrics
