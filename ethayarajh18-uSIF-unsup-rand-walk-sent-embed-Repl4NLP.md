Kawin Ethayarajh
uSIF: Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Basline
Rep4NLP@ACL 2018

unsupervised smoothed inverse frequency (uSIF)

# Abstract

* SIF [outperforms] LSTMs on textual similarity tasks
* we
  * show that word vector length has a confounding effect
    on the probability of a sentence ... in Arora+ ’s model
  * propose a random walk model that is robust to this confound
  * probability of word ... is inversely related 
    to the angular distance between the word and sentence embeddings
  * beats Arora+ ’s by up to 44.4% on textual sim & competitive with SOTA
  * no hyperparameter tuning

# 1 Intro

* methods ranging from a weighted average of word embeddings to convolutional,
  recursive, and recurrent neural networks 
  (Le and Mikolov, 2014; Kiros+ 2015; Luong+ 2013; Tai+ 2015)
* sophisticated architectures are often outperformed, particularly in transfer
  learning settings, by sentence embeddings generated as a simple average of
  tuned word embeddings (Wieting+ 2016b)
* Arora+ (2017) provided a more powerful approach: compute the sentence
  * ‘weakly supervised’, since it requires hyperparameter tuning (Cer+ 2017).
* we first propose a class of worst-case scenarios for Arora+ ’s (2017) rand wk
  * sentence g that is dominated by words with zero similarity, and some
  * sentence h that is dominated by identical words, long vec

* contributions in this paper are three-fold.  First, 
  * [new] random walk model that is robust to distortion by vector length,
  * probability of a word vector being generated by a discourse vector is
    inversely related to the angular distance between them. 
  * MAP estimate for the sentence embedding as follows: 
    normalize the word vectors, take a 
    weighted average of them, and then 
    subtract ... the projection on their first m principal components. We call
  * outperforms Arora+ ’s by up to 44.4% on textual similarity tasks, and is
    * competitive with SOTA methods.
