Pretrained Language Models for Text Generation: A Survey
Junyi Li, Tianyi Tang, Wayne Xin Zhao, Ji-Rong Wen
IJCAI 2021 Survey Track arXiv:2105.10311 [cs.CL]

* how to adapt existing PLMs to model different input data and 
  satisfy special properties in the generated text. We further summarize
* fine-tuning strategies for text generation. Finally, we present several

# 1 Introduction

* Existing surveys in this area have only partially reviewed some related topics
  * Zaib+ [2020] and Guan+ [2020]: dialogue systems and summarization, but did

# 2 general task definition and different text generation tasks

# 3 mainstream architectures of PLMs that are used in text generation

# 4 Modeling Different Data Types from Input

# 5 Satisfying Special Properties for Output Text

# 6 fine-tuning strategies for text generation 4

* we review several commonly-used fine-tuning strategies from different views

# 7 future directions and conclusion
