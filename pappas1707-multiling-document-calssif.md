Multilingual Hierarchical Attention Networks for Document Classification
Nikolaos Pappas, Andrei Popescu-Belis
arXiv:1707.00896 [cs.CL]

* Hierarchical attention networks have recently achieved remarkable performance
  for document classification in a given language. However, when multilingual
* multilingual model with fewer parameters and cross-language transfer is
* we propose multilingual hierarchical attention networks for learning document
  structures, with shared encoders and/or shared attention mechanisms across
  languages, using multi-task learning and an aligned semantic space as input.
* We evaluate the proposed models on 
  multilingual document classification with disjoint label sets, on 
  * a large dataset which we provide, with 
    600k news documents in 8 languages, and 5k labels. The 
    * English, German, Spanish, Portuguese, Ukrainian, Russian, Arabic, Persian
* multilingual models outperform monolingual ones in low-resource as well as
  full-resource settings, and use fewer parameters, thus confirming their
