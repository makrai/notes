Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

# Abstract

* Bidirectional ... Transformers. Unlike recent language representation models,
* jointly conditioning on both left and right context in all layers. As a
* can be fine-tuned with just one additional output layer to create SOTA models
  for a wide range of tasks, such as question answering and language inference,
