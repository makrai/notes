Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra
arXiv:2201.02177 [cs.LG]

* study generalization of neural networks
  on small algorithmically generated datasets
  * data efficiency, memorization, generalization, and speed of learning 
    * In this setting, questions these can be studied in great detail
* In some situations we show that neural networks learn through a process of
  "grokking" a pattern in the data, improving generalization performance from
  random chance level to perfect generalization, and that
  * this improvement in generalization can happen well past the point of
    overfitting 
  * generalization as a function of dataset size: 
    smaller datasets require increasing amounts of optimizat for generalizat
  * these datasets provide a fertile ground for studying a poorly understood
    aspect of deep learning: generalization of overparametrized neural networks
    beyond memorization of the finite training dataset. 
