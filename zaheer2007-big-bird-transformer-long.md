Big Bird: Transformers for Longer Sequences
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti,
  Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed
arXiv:2007.14062 [cs.LG]

# Abstract

* BERT's limitations is the quadratic dependency (mainly in terms of memory) on
  the sequence length due to their full attention mechanism. To remedy this,
* we propose, BigBird,
  * a sparse attention mechanism that reduces this to linear We show that
  * universal approximator of sequence functions and is Turing complete,
  * O(1) global tokens (such as CLS), that attend to the entire sequence as
  * can handle sequences of length up to 8x of what was previously possible
  * drastically improves performance on [] question answering and summarization
  * novel applications to genomics data
